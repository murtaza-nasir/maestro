# Governing Public Sector Automated Decisions: Balancing Efficiency, Justice, and Trust

# 1. Introduction

Automated decision-making (ADM) systems are increasingly being adopted within the public sector, promising enhanced efficiency, consistency, and cost-effectiveness in service delivery and administrative processes. However, the deployment of these systems introduces complex tensions, particularly between the pursuit of algorithmic efficiency and the imperative of ensuring procedural justice for citizens. Algorithmic efficiency, often measured by speed, scalability, and resource utilization, can sometimes conflict with the requirements of procedural justice, which demands fairness, transparency, accountability, and the ability for individuals to understand and contest decisions that affect them. This report examines the critical role of governance mechanisms in navigating this tension, aiming to balance the benefits of automation with the fundamental principles of justice, while simultaneously maintaining democratic accountability and fostering citizen trust in public institutions.

The increasing complexity and opacity of some ADM systems, often referred to as algorithmic blackboxing, pose significant challenges to achieving procedural justice. This opacity can arise from technical factors, but is also deeply embedded in organizational practices and institutional frameworks that may inadvertently or intentionally obscure the inner workings and impacts of these systems. Such lack of transparency and understanding can hinder the identification and mitigation of biases, limit opportunities for effective human oversight, and complicate the assignment of accountability when errors or injustices occur. These issues directly impact citizens' ability to perceive the process as fair and to trust the decisions made by or with the assistance of these systems.

To address these challenges, a range of governance mechanisms is necessary. These include technical controls designed to enhance transparency and auditability, organizational structures and processes that ensure internal accountability and human oversight, and strategies for engaging external stakeholders to incorporate diverse perspectives and build legitimacy. Effective governance frameworks must bridge the gap between the technical capabilities of ADM and the broader societal and ethical considerations inherent in public service. This report contributes to the literature by synthesizing these diverse governance mechanisms into a comprehensive framework that explicitly addresses their potential for balancing the tension between algorithmic efficiency and procedural justice, while also considering their role in enhancing democratic accountability and citizen trust in public sector ADM.

This report is guided by key questions concerning the intersection of algorithmic efficiency and procedural justice in public sector ADM. It explores how different governance mechanisms can mitigate the negative impacts of algorithmic opacity and bias, ensure meaningful human oversight and accountability, and ultimately contribute to building and maintaining citizen trust. By synthesizing insights from existing literature, the report aims to provide a comprehensive overview of the challenges and potential solutions in governing automated decision systems in a manner that upholds democratic values and serves the public good.

The following section will provide a detailed review of the existing literature on algorithmic blackboxing and the current gaps in governance mechanisms.

# 2. Literature Review

This literature review synthesizes existing academic perspectives on governance mechanisms in automated decision-making systems, particularly within public sector contexts. It examines the complex challenge of algorithmic blackboxing, exploring its manifestation across technical, organizational, and institutional layers. Furthermore, the review identifies critical gaps in current governance frameworks and practices, highlighting areas where existing mechanisms fall short in addressing issues of transparency, accountability, procedural justice, and democratic oversight.

## 2.1. Algorithmic Blackboxing

Algorithmic decision-making (ADM) systems, despite their potential for efficiency, often suffer from opacity, making their internal logic difficult to comprehend [3]. This lack of transparency, often referred to as algorithmic blackboxing, hinders the ability to establish accountability, accurately assess the robustness and reliability of system outputs, and consequently erodes trust in these technologies [3].

Algorithmic blackboxing extends beyond mere technical complexity. Research indicates it is a multilayered phenomenon in public sector ADM, driven by deliberate organizational and institutional practices [6]. Intra-organizational blackboxing occurs when public agencies refrain from addressing early warnings, understanding software details, scrutinizing results before communication, or confronting critical information, leading to internal self-imposed blindness [6]. Extra-organizational blackboxing involves actively preventing external insight through actions such as not formalizing procurement decisions, withholding information on system use despite requests, and buffering critique through audits or dialogues with restricted scopes [6]. Furthermore, institutional blackboxing arises when legal and policy frameworks fail to adapt to the nature of ADM, such as courts dismissing algorithmic agency, supporting outdated procedures, or displaying blindness to new error categories and power imbalances [6]. This resistance to adapting institutions and legal norms to an algorithmic context can result in legal injustice [6].

The combined effect of these layers of blackboxing has significant implications for procedural justice and citizen trust. The obscuring of decision-making processes through these ignoring practices can lead to social injustice, including the unfair distribution of resources [6]. Public service recipients can be misrepresented and excluded, facing barriers to understanding how ADM affects them and struggling to challenge defective algorithms legally [6]. This lack of transparency and inability to contest decisions undermines procedural fairness, which is perceived when processes meet criteria such as accuracy, lack of bias, correctability, and ethicality [1].

The presence of bias within opaque algorithmic systems further exacerbates issues of procedural justice, negatively impacting users' perceptions of fairness and potentially leading to discrimination [1]. Discriminatory outcomes are widely viewed as unfair and can provoke strong negative reactions [1]. Achieving social consensus on the ethicality of algorithmic decisions is particularly challenging in opaque situations, underscoring the necessity of algorithmic transparency to determine the appropriateness of deployment [1].

Citizen trust in algorithmic systems, defined as confidence and willingness to act based on recommendations [1], is heavily influenced by transparency. Humans tend to trust automation more if its internal workings are transparent, explainable, and understandable [1]. Conversely, system outputs that are not well-explained decrease trust [1]. Observations of procedural unfairness, including biased recommendations or the use of sensitive variables, can negatively affect user trust in the system's competence and integrity [1][1]. The common expectation that machines are unbiased means that violating this assumption by using sensitive variables can be perceived as a service failure, negatively impacting trust [1]. When algorithmic performance is perceived as unfair due to inappropriate proxy variables, it can negatively impact trust in the system's reliability [1]. The Robodebt case in Australia serves as a stark example of how an opaque and poorly designed ADM system, lacking transparency and sufficient human oversight, led to widespread unfair treatment, distress, and ultimately eroded public trust in government services [3].

In summary, algorithmic blackboxing, encompassing technical opacity as well as organizational and institutional practices of ignoring, significantly obscures decision-making processes in public sector ADM. This opacity directly impedes accountability, undermines procedural justice by hindering fairness perceptions and the ability to contest decisions, and erodes citizen trust in the systems and the institutions deploying them. The following section will examine existing gaps in governance mechanisms intended to address these challenges.

## 2.2. Gaps in Governance Mechanisms

Existing governance mechanisms for automated decision-making (ADM) systems in the public sector exhibit significant gaps, particularly in addressing the complex interplay between algorithmic efficiency and procedural justice. A primary area of deficiency lies in the failure of public institutions and legal systems to adequately respond to the negative consequences of ADM, allowing algorithmic injustices to persist uncorrected for extended periods [6]. This failure to address consequences directly undermines procedural justice by denying affected individuals recourse and correction, and it erodes citizen trust in the institutions deploying these systems. This is not solely a technical issue but is deeply rooted in organizational and institutional practices [6].

Organizational ignoring practices contribute significantly to a multilayered blackboxing of ADM systems [6]. These practices include actively avoiding confrontation with ADM risks and consequences (avoiding), intentionally obscuring information about the system from external stakeholders (obscuring), and denying the need for institutions, such as courts, to adapt to the realities of ADM (denying) [6]. Such practices create intra-organizational, extra-organizational, and institutional blackboxing, respectively, hindering accountability and leading to social and legal injustice [6]. This highlights a gap where existing legal and administrative frameworks fail to ensure compliance and correct errors, as illustrated by cases where even rule-based algorithms produced thousands of erroneous decisions that remained unaddressed by relevant authorities [6]. This failure to correct errors due to institutional inertia fundamentally compromises procedural justice and diminishes public trust.

Furthermore, the legal system may be ill-equipped to restore social justice when algorithms cause harm, with courts potentially avoiding taking a stand on ADM issues by focusing on other case-specific factors [6]. This points to a gap in governance where existing societal structures and legal capabilities are insufficient to address and correct algorithmic injustices beyond the ADM-using organization itself [6]. Such limitations in legal recourse directly impact procedural justice and the ability to hold systems accountable. The concept of "multilayered blackboxing" underscores that governance gaps are not merely technical but are also embedded in organizational and institutional practices that prevent actors from seeing and addressing ADM errors [3cc998405]. The literature has predominantly focused on preventing ADM errors through technical design or human-in-the-loop approaches, giving less attention to how society and institutions react when errors occur, indicating a gap in governance research concerning institutional responses and capabilities for securing justice [6]. This research gap hinders the development of comprehensive governance strategies that can effectively address the efficiency-justice tension.

The Robodebt case in Australia serves as a critical empirical example of these governance gaps [3]. Driven by a narrow, efficiency-focused vision, the program employed an inadequately resourced and expert-deficient ADM system that minimized human agency [3]. This led to unintended negative outcomes, including discrimination, at scale [3]. This pursuit of efficiency at the expense of human oversight severely compromised procedural justice. Despite clear signals of destructive consequences, the system was sustained due to path dependency and escalation of commitment [3]. The case revealed critical deficiencies in existing legal frameworks and oversight mechanisms, as inquiries failed to address the core legal issue of reversing the onus of proof [3]. This lack of legal clarity and effective external oversight demonstrates a significant gap in accountability and eroded citizen trust. This underscores the need for effective internal governance and oversight within deploying organizations to compensate for external deficiencies [3].

A lack of clarity exists regarding how accountability for the consequences of ADM use would be realized in practice, particularly with multiple stakeholders involved [20]. Knowledge is limited on how organizations manage legal liability and decide responsibility for consequences [20]. This represents a significant governance gap, especially in safety-critical environments or when providing services to vulnerable populations [20]. This lack of clear accountability directly impedes the ability to ensure justice when errors occur and undermines citizen trust by creating uncertainty about who is responsible.

Moreover, existing public sector IT governance, heavily influenced by historical models focused on efficiency, struggles to effectively govern and foster innovation [17]. This can lead to resources being directed towards low-risk, cost-cutting investments rather than exploring new opportunities, potentially creating an imbalance between efficiency goals and the need for exploration and adaptation [17]. This historical bias towards efficiency in governance can hinder the adoption of mechanisms necessary to ensure procedural justice and adapt to the ethical challenges of ADM.

The fundamental difference between the formal rationality of AI and the substantive rationality of humans also highlights a governance gap [8]. AI struggles with contextual nuances and value-laden data, leading to biased outcomes [8]. Governance mechanisms that rely solely on technical fixes or formal rules are insufficient, pointing to the need for frameworks that incorporate human oversight and acknowledge AI's limitations in contexts requiring complex value judgments [8]. This gap directly impacts procedural justice by allowing decisions based on incomplete understanding of human context, and it can erode trust if decisions are perceived as unfair or nonsensical.

Lack of AI governance, poor data quality, and inefficient training contribute to "AI-BA opacity," leading to suboptimal decisions and increased perceived risk [7]. This opacity hinders understanding, making it difficult to identify fairness-related issues and exacerbating the tension between efficiency and procedural justice [7][1]. Without proper governance, flawed data can be acquired, and employees may not effectively use the system, causing harm [7]. These issues directly impact the reliability and fairness of ADM outputs, undermining both efficiency due to poor decisions and procedural justice due to potential inaccuracies and biases, ultimately damaging citizen trust.

The absence of specific governance mechanisms for strategic and innovative AI projects in public sector organizations contributes to difficulties in managing feasibility, resources, risks, and effects [14]. Legal challenges related to data privacy regulations can necessitate resource-intensive manual processes, highlighting a gap where existing legal frameworks may hinder efficiency while aiming to uphold justice [14]. Furthermore, a lack of AI/ML competence among employees and managers, coupled with challenges in translating AI policies to an operational level, indicates a governance gap in workforce training and policy implementation [14]. These competence and implementation gaps hinder the effective deployment of governance mechanisms needed to balance efficiency and justice, and they can lead to errors that damage trust. The potential for algorithmic outputs to conflict with established norms, ethics, or practices ("normative divergence") represents a governance gap where mechanisms are needed to ensure alignment with public service values [14]. This divergence directly impacts procedural justice by producing outcomes that conflict with societal expectations and ethical principles.

Finally, there is limited research on how organizations design governance arrangements for combining human judgment and ADM, deciding the balance of control, and determining the necessity of human involvement in the decision-making loop [20]. This gap is particularly critical in sensitive public sector contexts and directly impacts the ability to ensure human oversight is effectively integrated to uphold procedural justice. Issues such as the potential for digital identity systems to introduce risks like data breaches and violations of data justice norms also highlight governance challenges, especially when balancing planned technical decisions with emergent outcomes requires flexibility that may conflict with the high disclosure needed for ethical technology [12]. The lack of consideration for the moral and ethical implications in certain approaches to digital systems orchestration in the public sector further points to a governance gap [12]. These gaps in addressing ethical implications and balancing flexibility with disclosure undermine both procedural justice and citizen trust.

In summary, significant gaps in current governance mechanisms for public sector ADM include institutional failure to address algorithmic harms, insufficient legal and administrative frameworks for correction and accountability, a lack of clarity on practical accountability and liability, an overemphasis on efficiency in IT governance hindering adaptation and exploration, insufficient frameworks for integrating human substantive rationality, deficiencies in managing data quality and training, a lack of specific governance arrangements for human-AI collaboration and ethical alignment, and challenges in ensuring ethical considerations are central to digital system orchestration. These gaps contribute to the persistence of algorithmic injustice and exacerbate the tension between efficiency and procedural justice, while simultaneously hindering democratic accountability and eroding citizen trust. The following section will delve into specific governance mechanisms that can potentially address these identified gaps.

# 3. Governance Mechanisms

No information found to write this section.

## 3.1. Technical Mechanisms

Technical mechanisms play a crucial role in enhancing the transparency and auditability of automated decision systems, contributing to procedural justice. Algorithmic transparency, audit trails, and explainability frameworks are key technical approaches.

Algorithmic transparency, such as Feature Importance (FI) transparency, which details the features and their weight in an algorithm's decision, can increase users' moral awareness and reduce the adoption of recommendations exhibiting direct discrimination [1]. When direct discrimination is present, FI transparency has been shown to increase moral awareness and decrease trust in the system, mediated by perceived fairness [1]. Users receiving FI were less likely to approve biased recommendations with direct discrimination [1]. However, FI alone is less effective in addressing indirect discrimination [1]. Augmenting FI with Algorithmic Demographic Information (ADI), which illustrates variable distribution across demographic groups, helps users identify indirect discrimination and reduces the likelihood of approving such recommendations by increasing moral awareness [1]. ADI enables users to discern if a variable unfairly disadvantages a particular group [1]. This suggests that transparency revealing bias negatively impacts perceived fairness, leading to a reduction in the adoption of biased recommendations [1]. The effectiveness of transparency mechanisms is highly situation-specific, indicating that a "one size fits all" approach is not suitable [1]. While revealing direct discrimination can decrease trust, the impact on trust is less clear when indirect discrimination is present, potentially because users may attribute bias to the data rather than the system [1]. Appropriate transparency is crucial for triggering users' moral awareness and influencing ethical decision-making when interacting with algorithmic systems [1]. Transparency can reveal potential discrimination areas that might otherwise remain hidden [1]. The rationale is that understanding algorithmic workings allows human users to identify fairness issues and take corrective action [1]. However, human cognitive biases can complicate the interpretation of algorithmic outcomes even with transparency [1].

Audit trails serve as technical mechanisms for tracing and reviewing system actions, providing a form of "emulated accountability" necessary for analyzing operational thoroughness and identifying misuse [2]. For instance, an audit trail mechanism developed for a software robot system processing sensitive data logged inputs, triggers, and invocation times, enabling monitoring, verification of data processing, and troubleshooting [2]. This mechanism, while adding complexity, had negligible impact on efficiency and supported technical troubleshooting and compliance, aligning with security-by-design principles emphasizing visibility and transparency [2]. Audit trails are considered foundational for security design, empowering users with tools for proactive security involvement and issue identification [2]. In a public sector context, implementing robust audit trails for automated benefits eligibility systems or predictive policing tools allows for post-hoc analysis of individual decisions and system-wide patterns, crucial for identifying potential biases or errors and demonstrating accountability. While audit trails can enhance accountability, implementing highly detailed logging across millions of transactions in a large-scale public service delivery system could potentially introduce storage overhead or minor processing delays, representing a potential, albeit often negligible, efficiency trade-off.

Explainability frameworks, particularly in domains like Process Outcome Prediction (POP), address the interpretability and faithfulness of predictive models [18]. Interpretability refers to the ability to provide explanations consisting of single, human-understandable chunks of information [18]. Faithfulness is defined as the accuracy with which an explainability model mimics the predictive model's behavior, not just its predictions [18]. While there is a trend towards using models from the explainable artificial intelligence (XAI) field, evaluation often prioritizes predictive performance over explainability and actionability [18]. Existing explainability metrics can be model-dependent and less suited for process-based analysis [18]. Challenges exist with the faithfulness of post-hoc methods like LIME and SHAP in POP, showing low-to-moderate faithfulness scores [18]. There is a need for model-agnostic metrics tailored to process-based analysis that assess both interpretability and faithfulness [18]. Research indicates a trade-off between predictive performance and both the interpretability of explanations and the faithfulness of the explainability model [18]. Prioritizing high predictive performance can negatively impact interpretability and faithfulness, particularly for black-box models [18]. Public sector applications are exploring the use of XAI techniques to provide explanations for decisions made by complex models, for example, in risk assessment for social services or loan applications, though ensuring these explanations are both accurate (faithful) and understandable (interpretable) to case workers and citizens remains a challenge. Developing models that are inherently more interpretable, or implementing post-hoc explainability techniques, might require additional computational resources or model constraints that could slightly reduce the maximum achievable predictive performance compared to a purely optimized black-box model, illustrating a potential trade-off with efficiency for enhanced understanding.

Despite their potential, technical mechanisms face practical limitations in public sector implementation. Challenges include inadequate data quality and misalignment between available open data and user needs, as seen in the difficulty of providing raw, flexible data access through open data platforms [22]. Technical capacity limitations, such as the processing burden on legacy systems and architectural constraints making certain features cost-prohibitive, also hinder implementation [22]. Furthermore, challenges related to user understanding arise, including designing interfaces and data access methods that cater to varying levels of technical expertise and data requirements [22]. Beyond technical hurdles, organizational and institutional practices, such as ignoring risks, obscuring information, and denying the need for institutional adaptation, can create multilayered blackboxing that limits the effectiveness of technical transparency and explainability by preventing external scrutiny and hindering legal recourse [6]. Even technically simple algorithms can become opaque due to these practices [6]. Ethical challenges associated with AI-based decision-making, such as objectivity, privacy, transparency, accountability, and trustworthiness, are also highlighted, noting that AI alone lacks the necessary preconditions for ethical decisions [15][15]. Critical issues requiring further research include factors affecting the interpretation of AI results and the consequences of AI use [15].

Having explored technical mechanisms and their limitations, the following section will examine organizational mechanisms for governance.

## 3.2. Organizational Mechanisms

No information found to write this section.

### 3.2.1. Internal Accountability Structures and Processes

Establishing robust internal accountability structures is critical for organizations deploying automated decision-making (ADM) systems, particularly within the public sector, to enhance transparency and build trust among stakeholders. While human stakeholders are generally considered ultimately responsible for the consequences of ADM decisions [20], the practical realization of this accountability is complex due to the involvement of diverse stakeholders across potentially multiple organizations [20]. There is a recognized gap in understanding how organizations design governance arrangements for ADM and manage the associated legal liability [20].

Effective internal governance requires organizations to proactively adopt best-practice frameworks, especially in the absence of comprehensive external governance and legal oversight [3]. These frameworks should encompass ensuring appropriate human oversight, identifying, mitigating, and monitoring risks before and during system deployment through mechanisms like impact assessments and ethical review boards [3]. Rigorous checks on algorithmic accuracy and robustness prior to deployment are also essential [3]. Furthermore, establishing effective independent investigation processes to address complaints is a key component of internal accountability [3]. Specific examples of such structures and processes in public sector contexts include the development of internal ethical review boards to scrutinize proposed ADM deployments, the mandatory use of algorithmic impact assessments to identify potential risks to individuals and groups, and the establishment of dedicated data governance boards responsible for overseeing data quality and usage in ADM systems [3]. Beyond these, internal accountability can be strengthened through the creation of specific roles such as a data ethics officer or an algorithmic fairness ombudsperson within the organization. Process steps like mandatory pre-deployment testing for bias and discrimination, documented decision protocols for human-in-the-loop scenarios, and post-implementation monitoring protocols with defined review cycles are also vital components of a robust internal accountability framework. Successful implementation of such frameworks necessitates that public agencies develop sufficient internal capacity and competency in both technical and human resources to manage ADM programs responsibly [3]. The Robodebt case in Australia highlighted the critical need for effective internal governance and oversight functions to compensate for deficiencies in legal frameworks and external oversight [3].

Organizational practices can significantly impact internal accountability by creating "intra-organizational blackboxing" [6]. This occurs when organizations fail to adequately understand or scrutinize their ADM systems, hindering the ability to identify and address risks and consequences [6]. To counter this, public agencies utilizing ADM should be held accountable, both individually and organizationally, for explaining the system's use, risks, and consequences [6]. This includes formal responsibility for monitoring regulatory compliance, risks, and consequences [6]. Organizations must also take responsibility for verifying that ADM system code and processes comply with regulations and, where necessary, translate regulations into code [6]. Such actions foster internal engagement with ADM, enabling prevention, early detection, and correction of detrimental outcomes, thereby enhancing internal accountability and transparency [6].

Case studies reveal challenges in establishing effective internal accountability. One case highlighted a lack of structures for knowledge management and low overall AI competence, creating difficulties in preparing results for staff use [14]. The absence of governance mechanisms for strategic and innovative AI projects can lead to difficulties in navigating feasibility, resource management, risks, and effects [14]. Furthermore, a lack of AI/ML competence among employees and managers, coupled with challenges in translating AI policies to an operational level, underscores the importance of educating top managers about the fundamental capabilities and limitations of AI to "demystify" it [14]. Developing internal frameworks for sustainable and responsible AI, aligned with the organization's public service ethos and overall strategies, is crucial for maintaining public trust [14]. Coordinating AI work with other organizational initiatives helps avoid silos and addresses challenges in scaling, data reuse, and quality that require organized data management [14]. Ensuring that algorithmic practices are attuned to overall strategies and the public service ethos, accounting for the contextual nature of organizational data, and developing human-centric ML systems that allow continuous involvement of domain experts are vital for effective and responsible use of data-driven predictive systems [14].

The opacity of advanced algorithms can make it difficult for even internal managers and employees to understand why a specific decision was made, leading to perceptions of unfairness and potentially impacting perceived procedural fairness and organizational commitment [19]. This opacity complicates the assignment of accountability for mistakes or misconduct [19]. Organizations must accept responsibility for the outcomes and ethical implications of algorithmic decisions, even if the underlying system operates as a black box [19]. Relying solely on autonomous systems without robust internal accountability could lead organizations to avoid responsibility for potentially significant decisions [19].

Maintaining trust among internal stakeholders, particularly employees, is influenced by the transparency of algorithmic decision aids [1]. When transparency reveals the use of sensitive variables or direct discrimination, trust can decrease, mediated by perceptions of fairness [1]. This suggests that employees may hold algorithmic systems accountable for justice, potentially with different standards than for human decision-makers [1]. Organizations must be aware that employees might accept embedded biases in algorithmic recommendations [1]. Providing appropriate transparency within the organization is crucial for accountability and preventing problematic outcomes, but requires careful consideration as a "one size fits all" approach is ineffective [1].

In summary, effective internal accountability structures in public sector organizations deploying ADM require proactive governance frameworks, sufficient internal capacity, a commitment to countering intra-organizational blackboxing through transparency and responsibility, and addressing challenges related to AI competence and alignment with organizational values. These internal mechanisms, including specific roles, committees, and process steps, are essential for building transparency and fostering trust among stakeholders. The following section will explore strategies for external stakeholder participation and engagement.

### 3.2.2. Stakeholder Participation and Engagement Strategies

Engaging diverse stakeholders in the governance of automated decision-making (ADM) systems is crucial for enhancing trust and accountability within the public sector. This involves strategies that go beyond traditional top-down approaches, incorporating the perspectives of citizens, civil servants, experts, and other relevant groups throughout the system lifecycle [5][16]. Effective engagement can play a vital role in balancing the pursuit of algorithmic efficiency with the requirements of procedural justice.

Strategies for stakeholder engagement draw from various fields, including public policy, operations research (OR), and digital transformation [5][11][16]. In the context of OR and public sector decision-making, approaches range from physical presence methods like facilitated workshops, collaborative activities, and problem-structuring methods (e.g., cognitive mapping, strategic choice approach) to digital tools such as e-participation platforms and online Multi-Criteria Decision Analysis (MCDA) tools [9][16]. The Multi-Actor Multi-Criteria Analysis (MAMCA) framework, for instance, is a stakeholder-based group decision-making approach that explicitly integrates diverse perspectives by allowing different stakeholder groups to evaluate alternatives based on their own criteria [9]. This highlights the importance of acknowledging and negotiating diverse "frames of interpretation" held by stakeholders, which are shaped by their experiences and motivations [5]. Achieving alignment among these interpretations is vital for successful digital transformation initiatives, including ADM implementation [5]. By incorporating diverse perspectives early in the design phase, engagement can help identify potential biases or unintended consequences that might undermine procedural justice, potentially saving resources and time required for later remediation, thus contributing to long-term efficiency.

Stakeholder involvement is instrumental in social decision-making contexts, such as urban planning and policymaking, which often involve multifaceted criteria and diverse perspectives [9]. Stakeholders contribute valuable knowledge, enhance the legitimacy and acceptance of decisions, improve decision quality by fostering innovative solutions and conflict avoidance, and increase the viability of outcomes by supporting implementation [9]. Engaging stakeholders transforms multi-criteria problems into group decision-making scenarios, requiring methods for eliciting and aggregating preferences [9]. Facilitated modelling, where practitioners guide groups through structured problem-solving using tools like problem structuring methods and decision analysis, is a critical approach within stakeholder-based frameworks to ensure perspectives are incorporated effectively [9]. Different frameworks vary in their aggregation levels, from input-level (seeking consensus on elements for a unified ranking) to output-level (segmenting stakeholders and aggregating preferences separately) [9]. MAMCA offers flexibility, blending elements of both, and is used in sectors like transportation, energy, and environment, including citizens, policymakers, and decision-makers as key groups [9]. This integration of diverse viewpoints directly supports procedural justice by ensuring that the values and concerns of those affected by ADM are considered, moving beyond purely technical or efficiency-driven criteria.

Specific strategies for engaging stakeholders in the context of ADM governance can draw lessons from related participatory processes. For example, methodologies involving expert panels and independent experts in structured decision-making processes, even if not directly related to ADM, demonstrate the value of forming expert groups, conducting consolidation sessions for feedback and validation, using inclusive and participatory methods, employing facilitators to guide participants unfamiliar with techniques, and incorporating diverse perspectives [4]. The transparency of results and facilitated interpretation of models, achieved through expert feedback, can help validate decision-support systems and enhance trust [4]. Inclusive and participatory methods themselves aim to enhance understanding among participants, which is a principle applicable to building trust in ADM governance [4]. Valuing and incorporating expert knowledge also contributes to the credibility and acceptance of the process and its outcomes [4]. By making the ADM process and its outcomes more understandable and incorporating external validation, these strategies directly build trust in the *system* itself, not just the public body deploying it.

Stakeholder involvement can enhance citizen trust by increasing the legitimacy and acceptance of decisions [9]. When stakeholders are involved, they are more likely to support and sustain decision outcomes, critically influencing successful implementation [9]. Acknowledging the role of stakeholders improves decision implementation and addresses ethical considerations of fairness and inclusivity [9]. For instance, participatory design processes where citizens co-design fairness metrics or establish feedback loops can directly address perceptions of procedural justice by ensuring the criteria and processes align with community values. Transparency of results and the ability for facilitated interpretation of model outcomes, as demonstrated in expert validation processes, directly contribute to building trust by making the system's outputs understandable and verifiable [4]. By contributing to a perception of fairness and transparency, engagement mechanisms foster trust in the ADM system's competence and integrity. However, research indicates a gap in specifically detailing how these engagement mechanisms enhance citizen trust in *automated systems* and providing frameworks for evaluating their effectiveness regarding procedural justice and democratic accountability [4][16].

Challenges in ensuring meaningful and inclusive stakeholder engagement in complex ADM contexts include initial unfamiliarity with technical concepts and methods, logistical issues with remote participation, context dependency of findings from participatory processes, and the significant time commitment required [4]. Best practices to mitigate these challenges include using facilitators to guide participants, designing inclusive methods that enhance understanding, encouraging active participation, incorporating diverse expertise, conducting consolidation/validation sessions with independent parties, and ensuring transparency in communicating results [4].

While stakeholder engagement is recognized as critical, there is limited research on how organizations design governance arrangements for ADM decision-making in different contexts and how accountability is realized in practice across multiple stakeholders and organizations [20]. Future research should explore how organizations decide responsibility for ADM consequences and the characteristics of 'responsible' ADM systems, potentially using mixed-methods approaches including stakeholder surveys and focus groups [20].

In summary, various strategies exist for engaging stakeholders in public sector ADM governance, ranging from traditional workshops to digital platforms, with frameworks like MAMCA offering structured approaches for incorporating diverse perspectives. Effective engagement enhances decision legitimacy, acceptance, and quality, contributing to trust and addressing ethical considerations by directly influencing perceptions of fairness and transparency. By proactively identifying potential issues through diverse input, engagement supports procedural justice and can enhance the long-term viability and efficiency of ADM deployments by fostering acceptance and avoiding costly conflicts. However, challenges remain in navigating technical complexity and ensuring clear accountability structures, highlighting the need for further research into the specific impacts of engagement on trust in ADM and the practical realization of accountability across stakeholders.

Having examined strategies for stakeholder participation and engagement, the following section will discuss the implications of these governance mechanisms for balancing efficiency and procedural justice.

# 4. Discussion

No information found to write this section.

## 4.1. Implications for Balancing Efficiency and Procedural Justice

Examining the diverse governance mechanisms, both technical and organizational, reveals their significant implications for navigating the inherent tension between algorithmic efficiency and procedural justice in public sector automated decision-making (ADM). While ADM systems are often implemented with the explicit goal of enhancing efficiency, quality, and consistency [6], their deployment can inadvertently introduce biases, exacerbate inequalities, and lead to discriminatory outcomes, thereby undermining social and legal justice [6]. The challenge lies in designing and implementing governance frameworks that can support the pursuit of efficiency without sacrificing the fundamental principles of fairness, equity, and accountability.

Governance mechanisms introduce specific implications for the balance between efficiency and procedural justice, often involving inherent trade-offs. Technical mechanisms, such as algorithmic transparency and audit trails, can directly support procedural justice by increasing the interpretability and traceability of ADM processes. Transparency mechanisms, like Feature Importance (FI) and Aggregated Demographic Information (ADI), enable users to identify potential sources of direct and indirect discrimination [1]. This increased visibility is crucial for fostering users' moral awareness and facilitating ethical decision-making [1]. Audit trails provide a record of system actions, enabling monitoring, verification, and troubleshooting, contributing to operational thoroughness and identifying potential misuse [2]. These technical tools, by making the decision process more understandable and auditable, contribute to the perceived fairness of the system, a key component of procedural justice. However, implementing highly granular transparency or extensive audit trails, while enhancing justice, could potentially introduce computational overhead or complexity that slightly reduces peak algorithmic efficiency in certain high-throughput applications. For example, generating detailed explanations for every individual decision or maintaining comprehensive, easily accessible audit trails for millions of transactions could require significant processing power and storage, potentially impacting system speed or cost. Conversely, well-designed technical governance can improve efficiency in the long run by reducing errors, mitigating risks, and streamlining compliance processes. The effectiveness of technical mechanisms is contingent on appropriate design and implementation, as a "one size fits all" approach to transparency is not suitable [1], and human cognitive biases can still influence the interpretation of algorithmic outcomes [1]. Furthermore, prioritizing predictive performance in model development can lead to trade-offs with interpretability and faithfulness, particularly for complex black-box models [18].

Organizational mechanisms, including internal accountability structures and stakeholder engagement strategies, address the socio-technical dimensions of balancing efficiency and justice. Establishing robust internal governance frameworks with clear human oversight, risk assessment processes (like impact assessments), and independent complaint mechanisms is essential, especially in the absence of comprehensive external oversight [3]. These internal controls aim to prevent and mitigate detrimental outcomes arising from ADM, thereby protecting procedural justice [3]. Counteracting intra-organizational blackboxing by holding public agencies accountable for explaining ADM use, risks, and consequences is vital for fostering internal engagement and enabling early detection and correction of errors [6]. Similarly, engaging external stakeholders, including citizens and civil society, through participatory methods can enhance the legitimacy and acceptance of ADM decisions, improve decision quality by incorporating diverse perspectives, and increase the viability of outcomes [9]. This collaborative approach contributes to procedural justice by ensuring that the values and concerns of those affected by ADM are considered in its design and deployment. While these organizational mechanisms primarily support justice, they can also indirectly impact efficiency. For instance, extensive stakeholder consultations, complex internal review processes, or mandatory human review steps might add time and resource overhead in the short term, potentially slowing down decision throughput compared to a fully automated process. However, by improving decision quality, increasing public acceptance, and preventing costly errors or legal challenges (as seen in the Robodebt case [3]), they can lead to greater long-term efficiency and sustainability by avoiding the significant costs associated with rectifying errors and regaining public trust.

However, the implementation of these governance mechanisms faces significant challenges that can impact the balance between efficiency and justice. Organizational ignoring practices, which contribute to multilayered blackboxing, can undermine the effectiveness of both technical and organizational controls by preventing internal and external scrutiny and hindering legal recourse [6]. The Robodebt case illustrates how a focus on efficiency and cost-cutting, coupled with a lack of robust internal and external governance, can lead to systems that cause significant harm and injustice [3]. Furthermore, the legal system may be ill-equipped to address algorithmic injustices, highlighting a gap where institutions struggle to restore social justice when algorithms err [6]. A lack of clarity on accountability and liability for ADM outcomes across multiple stakeholders also complicates the practical realization of justice [20]. While governance mechanisms are intended to support both efficiency and justice, there is a need for frameworks that explicitly address how to achieve this balance in practice, moving beyond identifying problems to detailing actionable strategies [6]. The application of general data analytics governance concepts, such as structural, process, and relational mechanisms, provides a theoretical lens [10], but specific guidance is needed for tailoring these to the public sector context to ensure human oversight, accountability, and fairness while pursuing efficiency goals. The inherent tension means that achieving perfect balance is challenging; often, strengthening one aspect (e.g., adding human review for justice) may introduce friction to the other (e.g., reducing processing speed for efficiency), necessitating careful design choices based on the specific public sector context and the potential impact of the ADM system.

Ultimately, achieving a balance between algorithmic efficiency and procedural justice requires a comprehensive approach that integrates technical, organizational, and institutional governance mechanisms. These mechanisms must be designed not only to optimize performance but also to actively prevent and remedy algorithmic injustices, ensuring transparency, accountability, and meaningful stakeholder engagement. The following section will delve deeper into specific strategies for addressing algorithmic opacity, bias, and the lack of transparency.

## 4.2. Addressing Algorithmic Opacity, Bias, and Lack of Transparency

Addressing algorithmic opacity, bias, and the resulting lack of transparency in automated decision-making (ADM) systems within the public sector is critical for upholding procedural justice and maintaining citizen trust. Algorithmic bias is a socio-technical issue stemming from biased data, methodological issues, or the inclusion of protected variables or proxies, which can lead to discriminatory decisions [1]. Discrimination can manifest directly through the explicit use of protected characteristics or indirectly through seemingly neutral variables that disadvantage protected groups due to biased data or modeling [1]. Opacity, or "blackboxing," compounds these issues by making the decision-making process difficult to understand, hindering the identification and mitigation of bias [3][6].

Effective governance mechanisms are essential to mitigate these challenges. Transparency is a key governance principle for minimizing harm, improving trust, and supporting legal compliance in ADM systems [1]. Technical approaches to transparency include Feature Importance (FI) and Algorithmic Demographic Information (ADI) [1]. FI, which identifies the variables used by an algorithm and their weighting, can help pinpoint potential biases, particularly in cases of direct discrimination [1]. However, FI is less effective for indirect discrimination, which often arises from biased data or inadvertent modeling procedures [1][13]. ADI offers a complementary approach by depicting demographic differences in relation to important features, potentially revealing sources of indirect discrimination [1]. Studies show that combining FI with ADI can increase users' moral awareness and reduce the likelihood of approving recommendations contaminated by indirect discrimination [1]. This suggests that appropriate transparency can trigger moral awareness and influence ethical decision-making [1].

Beyond technical solutions, addressing opacity and bias requires tackling the organizational and institutional practices that contribute to "multilayered blackboxing" [6]. "Organizational ignoring practices," such as avoiding confrontation with ADM risks, obscuring information from external stakeholders, and denying the need for institutional adaptation, reinforce opacity and hinder accountability [6]. To counter this, governance must emphasize engagement and responsibility within public agencies, requiring accountability for explaining ADM use, risks, and consequences [6]. This includes verifying regulatory compliance and translating regulations into code where necessary [6]. Improving access to information and power balance for external stakeholders, for instance, by mandating public agencies to issue declarations of ADM use and provide information upon request, is also crucial [6].

Legal and institutional adaptation is also necessary to address institutional blackboxing and ensure legal recourse and restitution [6]. Empowering legal institutions to assess algorithm legality and hold actors accountable may require adapting laws and regulations, potentially including burden-of-proof reversal [6]. The creation of an ombudsperson for algorithmic justice could assist vulnerable groups in safeguarding their rights and counteracting power imbalances [6].

Challenges persist in implementing these governance mechanisms effectively. A "one size fits all" approach to transparency is not suitable [1], and balancing simplicity with complexity in transparency approaches is necessary to avoid cognitive burden and maintain trust [1]. Furthermore, organizational factors such as a lack of AI/ML competence among staff and managers, and challenges in translating AI policies to an operational level, can hinder the effective implementation of governance aimed at mitigating bias and opacity [14]. The fundamental difference between AI's formal rationality and human substantive rationality also presents a challenge, as AI struggles with contextual nuances and value judgments, contributing to bias even with seemingly bias-free data [8]. Governance frameworks must acknowledge this limitation and incorporate mechanisms that account for context and values [8].

A significant challenge lies in ensuring that transparency is not merely technical but also meaningful and actionable for citizens and non-expert stakeholders. The "transparency paradox" highlights that simply providing access to data or algorithmic details does not guarantee understanding, especially for complex systems [15]. Governance mechanisms must therefore address how to present information about ADM processes and outcomes in a way that is accessible, understandable, and relevant to the target audience. This may involve developing user-friendly interfaces, providing clear explanations of algorithmic decisions in plain language, and offering accessible avenues for citizens to query or challenge outcomes. For instance, public agencies could implement interactive online tools that allow citizens to input their specific case details and receive a simplified explanation of how the ADM system arrived at a particular outcome, highlighting the most influential factors without revealing proprietary code. Clear, easy-to-understand complaint and redress processes, distinct from traditional legal channels, are also essential to make transparency actionable. Governance should aim to bridge the gap between technical complexity and public comprehension, ensuring that transparency serves its intended purpose of enabling scrutiny and fostering trust.

Ultimately, effectively addressing algorithmic opacity and bias requires a comprehensive governance approach that integrates technical measures like targeted transparency with organizational practices that counter ignoring behaviors and institutional adaptations that ensure accountability and redress. This includes ensuring that transparency is designed to be understandable and actionable for all stakeholders, particularly the citizens affected by public sector ADM. The following section will delve into the critical aspect of ensuring human oversight and accountability in ADM systems.

## 4.3. Ensuring Human Oversight and Accountability

Ensuring appropriate human oversight and establishing clear accountability structures are paramount in the governance of automated decision-making (ADM) systems, particularly within the public sector. As ADM systems become more complex, potentially leading to opacity and a reduction in human interaction, expertise, and reflection, there is a risk of decisions being perceived as arbitrary or nonsensical [19]. This shift towards increased algorithmic agency necessitates robust governance mechanisms to keep humans appropriately involved in the decision-making loop [3].

The necessity of human oversight stems from the inherent limitations of algorithms, which lack the mindful, context-sensitive processing capabilities of humans and struggle with contextual nuances and value-laden data [3][8]. Over-reliance on algorithmic agency and minimizing human agency can lead to unintended negative outcomes at scale, especially when the system cannot handle the complexity of its context, potentially resulting in discrimination when humans are removed from the loop [3]. The Robodebt case serves as a stark example of the dangers of insufficient checks and balances and the removal of human agency in large-scale public sector ADM, highlighting the critical need for effective internal governance and oversight functions [3].

Different models of human oversight exist, each with varying implications for accountability and procedural justice. These include "human-in-the-loop," where a human makes the final decision based on algorithmic recommendations; "human-on-the-loop," where a human monitors the automated system and intervenes when necessary; and "human-out-of-the-loop" with periodic review, where the system operates autonomously but is subject to human audits and evaluations [6].

The suitability and practical implementation of each model in the public sector involve distinct considerations. The "human-in-the-loop" model, while potentially enhancing procedural justice by allowing for contextual judgment and exceptions, can be resource-intensive and potentially slow down decision processes, impacting efficiency, particularly in high-volume applications [3]. It requires significant investment in training human operators to effectively interpret algorithmic outputs and exercise discretion responsibly. The "human-on-the-loop" model offers a balance, allowing for greater efficiency by automating routine decisions while retaining the capacity for human intervention in complex or problematic cases. However, it necessitates robust monitoring systems and clear protocols for when and how human intervention should occur, as well as ensuring humans remain vigilant and do not overly rely on the system [6]. The "human-out-of-the-loop" model, while maximizing algorithmic efficiency, presents the greatest challenges for accountability and justice, requiring rigorous pre-deployment testing, continuous monitoring, and effective post-hoc auditing mechanisms to detect and correct errors or biases [6]. Cases exist where human involvement in these models did not prevent negative outcomes, indicating a need for further development of the "humans-in-the-loop" concept to ensure it genuinely adds safeguards, fairness, and transparency [6]. For high-stakes decisions or those involving vulnerable populations, models that retain meaningful human involvement in the decision process (human-in-the-loop) or provide continuous monitoring and intervention capabilities (human-on-the-loop) may be more appropriate to ensure fairness and allow for contextual exceptions [3].

Governance mechanisms play a crucial role in ensuring accountability in ADM systems. While human stakeholders are ultimately considered responsible for the consequences of ADM decisions [20], the practical realization of this accountability is complex, involving multiple stakeholders across potentially diverse organizations [20]. Organizations deploying ADM must proactively adopt best-practice frameworks that include ensuring appropriate human oversight, identifying, mitigating, and monitoring risks (e.g., via impact assessments, ethical review boards), checking algorithmic accuracy and robustness before deployment, and ensuring effective independent complaint investigation processes [3]. This requires public agencies to develop sufficient internal capacity and competency, investing in both technical and human resources, to run ADM programs responsibly [3].

However, establishing accountability is complicated by the opacity of complex algorithms, which can make it difficult to understand why a specific decision was made [19]. This opacity can obscure power structures and inhibit oversight, potentially leading organizations to avoid accountability by allocating decision-making authority to autonomous systems [19]. Despite this, organizations should be held responsible for the outcomes and ethical implications of these systems [19]. Holding ADM systems accountable requires more than just access to code or data; it necessitates a clear understanding of how the system functions and the ability to reconstruct the reasons for decisions [19].

Beyond technical opacity, organizational and institutional practices can contribute to "multilayered blackboxing," which hinders accountability [6]. Intra-organizational blackboxing, where ADM-using organizations avoid confronting risks and consequences, leads to self-imposed blindness and normalization of potentially problematic practices [6]. To counteract this, public agencies should have clear accountability, both individually and organizationally, for explaining ADM use, risks, and consequences, including formally monitoring regulatory compliance and verifying that code and processes correspond to applicable regulation [6]. Extra-organizational blackboxing, through intentional efforts to shield the algorithm from external stakeholders, also prevents scrutiny and reinforces social injustice [6]. Institutional blackboxing, resulting from denying the need to adapt institutions to ADM, can prevent appropriate legal responses and enable new injustices, such as misrepresentation in the legal arena [6].

Addressing these layered blackboxing mechanisms is crucial for ensuring accountability. It requires structural remedies like regulations, rules, and governance mechanisms that go beyond just technical explainability or basic "humans-in-the-loop" concepts [6]. There is a need to consider "institutional-safeguards-in-the-loop" as additional layers of protection against algorithmic misconduct [6]. Legal institutions need to be empowered to assess the legality of algorithms and their use and to hold actors accountable when ADM systems fail [6]. This involves adapting laws and competencies, potentially addressing the burden of proof and allowing for systemic decision reversal [6]. The creation of an ombudsperson for algorithmic justice could further support service recipients in safeguarding their rights [6].

Implementing effective human oversight and accountability mechanisms requires addressing challenges such as a lack of AI/ML competence among employees and managers and difficulties in translating AI policies to an operational level [14]. Educating top managers about AI's capabilities and limitations is essential [14]. Developing human-centric ML systems that allow continuous involvement of domain experts and ensuring algorithmic practices align with the public service ethos are vital [14]. Despite these needs, there is limited research on how organizations design governance arrangements for combining human judgment and ADM, deciding the balance of control, and determining the necessity of human involvement in the decision-making loop [20].

Ensuring human oversight and accountability in public sector ADM requires a multi-faceted governance approach that counters technical, organizational, and institutional blackboxing. This involves proactive internal governance frameworks, sufficient organizational capacity, addressing the limitations of algorithmic agency through appropriate human oversight models tailored to context and impact, empowering legal institutions, and developing human-centric systems with meaningful human involvement. The following section will explore how these elements contribute to building and maintaining citizen trust.

## 4.4. Building and Maintaining Citizen Trust

Building and maintaining citizen trust is paramount for the successful and ethical deployment of automated decision-making (ADM) systems within the public sector. Trust in algorithmic decision aids is defined as a user's confidence in and willingness to act based on the system's recommendations and decisions [1]. The success of algorithmic systems is widely acknowledged to depend on user trust [1]. This trust can be based on the system's performance (competency), its purpose (alignment with designer's intent), and its process (appropriateness of algorithms and ability to achieve goals) [1]. Process-based trust, in particular, emphasizes the importance of understandability in human-automation relationships [1]. Beyond these bases, citizen trust is also influenced by the perceived reliability and competence of the system and the public agency deploying it, as well as the effectiveness of communication strategies employed by public bodies [1][21].

Transparency and accountability are fundamental pillars for fostering citizen trust in ADM systems. Transparency is crucial for minimizing harm, improving trust, and supporting legal compliance [1]. Citizens tend to trust automation more if its internal workings are transparent, explainable, and understandable [1]. Conversely, system outputs that are not well-explained are less acceptable and decrease trust [1]. Explicability, encompassing system transparency and accountability, is considered a foundational principle of trustworthy AI [1].

Procedural fairness, which concerns the justice of the decision-making process itself, is a strong predictor of trust in organizations [1]. In the context of ADM, while algorithms are often perceived as inherently fair, the presence of biases that lead to unequal distribution of benefits or burdens negatively impacts users' fairness perceptions and can cause ethical outrage [1]. Observations of unfair conduct by the system can negatively affect trust, as users' belief in the system's process is a main foundation of trust [1]. Identifying biases in algorithms is likely to hamper users' trust in the system's competence and integrity, especially given the expectation that machines are unbiased [1]. A violation of this expectation, such as the use of sensitive variables, can be perceived as a service failure, negatively impacting trust [1].

Transparency mechanisms, such as Feature Importance (FI) and Demographic-based transparency (including Aggregated Demographic Information - ADI), play a vital role in enabling citizens to assess the fairness and reliability of algorithmic decisions [1]. FI reveals the variables used and their weighting, helping to pinpoint concealed biases and potentially impacting citizens' perceptions of fairness by allowing them to assess features based on relevance and reliability [1]. Providing FI when direct discrimination is present can increase citizens' moral awareness but may lower their trust in the system by revealing the use of sensitive variables and signaling procedural unfairness [1]. However, FI may be less effective in cases of indirect discrimination, potentially even increasing trust by falsely reassuring users [1]. Providing ADI alongside FI can reveal if important features have significantly different distributions across demographic groups, helping citizens see a lack of equality of opportunity and potentially increasing their moral awareness regarding indirect discrimination [1]. This additional transparency empowers citizens to identify features that negatively impact disadvantaged groups and better judge the fairness of algorithmic recommendations [1].

Beyond technical transparency, addressing organizational and institutional practices that contribute to "multilayered blackboxing" is crucial for building trust [6]. Practices that obscure information from external stakeholders and deny the need for institutional adaptation hinder citizens' ability to understand how they are affected by ADM and seek redress, leading to misrepresentation and misrecognition of public service recipients [3cc9405]. This prevents their democratic participation and ability to contest decisions, fundamentally undermining trust [6]. Counteracting these ignoring practices through governance mechanisms that promote access to information, power balance for external stakeholders, and legal recourse and restitution is essential [6]. For example, requiring public agencies to issue declarations of ADM use and provide information to service recipients and external parties enhances transparency and empowers citizens [6]. Empowering legal institutions and potentially creating an ombudsperson for algorithmic justice can help vulnerable groups safeguard their rights and address power imbalances [6].

Public agencies can proactively implement strategies to build and maintain citizen trust. This includes engaging in clear and effective communication about the purpose, function, and limitations of ADM systems to build confidence and manage expectations among citizens. Developing user-friendly interfaces for interacting with ADM systems and presenting predictions in an understandable way also contributes to trust by making the system's output accessible [14]. Understanding and quantifying model errors and openly discussing acceptable accuracy levels can be part of managing expectations and building trust [14]. Where feasible and appropriate, offering clear opt-out options for citizens regarding the use of ADM in non-critical decisions can empower individuals and foster trust. Furthermore, involving citizens in participatory design processes for ADM systems, for instance, in defining fairness metrics or establishing feedback loops, can directly address perceptions of procedural justice by ensuring that criteria and processes align with community values. Improving data readiness and implementing foundational data governance structures are necessary for effective ADM implementation and indirectly support trust through improved data quality and management [14].

The Robodebt case in Australia serves as a clear illustration of how the absence of robust governance, transparency, and accountability mechanisms can severely erode citizen trust [3]. The system's opacity, coupled with inadequate human oversight and governance deficiencies, led to widespread distress, unfair treatment, and ultimately jeopardized the implementing organization by realizing negative outcomes for stakeholders who could not opt out of the program [3]. This highlights the critical importance of setting up robust governance infrastructures for ADM programs to prevent socially destructive outcomes and maintain public trust [3]. Concerns regarding trust in AI for critical decisions, particularly in sensitive areas like those involving children, highlight potential limitations [14]. Furthermore, a lack of understanding among stakeholders due to the complexity of AI can hinder engagement and potentially erode trust [14].

The type of provider also influences trust, particularly in sensitive contexts. Users exhibit higher trust in public providers compared to private companies for the same service, even if public providers are perceived as having lower abilities [21]. The public mission to serve the public good aligns well with preserving sensitive data, contributing to higher trust in public providers compared to private companies whose primary focus is profit generation [21]. This institutional trust in public bodies can be a foundation upon which trust in public sector ADM systems can be built, provided that the systems are governed transparently and accountably, demonstrating competence and reliability.

In summary, building and maintaining citizen trust in public sector ADM systems requires a multi-faceted approach centered on transparency and accountability. This involves implementing technical transparency mechanisms tailored to reveal different types of bias, countering organizational and institutional practices that create opacity, empowering citizens with access to information and avenues for redress, leveraging the inherent institutional trust placed in public sector organizations, and proactively engaging in clear communication and participatory design processes about system capabilities and limitations.

The following section will synthesize these findings to provide a comprehensive conclusion.

# 5. Conclusion

This report has examined the critical tension between algorithmic efficiency and procedural justice in automated decision systems within public sector contexts, focusing on the role of governance mechanisms in navigating this complex landscape while maintaining democratic accountability and citizen trust. The literature review highlighted the pervasive challenge of algorithmic blackboxing, which extends beyond technical opacity to encompass deliberate organizational and institutional practices that obscure decision-making processes and hinder accountability. These practices contribute to significant gaps in existing governance frameworks, leaving public institutions ill-equipped to prevent, detect, and correct algorithmic injustices effectively.

The analysis of governance mechanisms revealed a range of technical and organizational approaches aimed at addressing these challenges. Technical mechanisms, such as targeted algorithmic transparency and audit trails, can enhance the interpretability and traceability of ADM systems, enabling the identification and mitigation of biases and supporting procedural fairness. Organizational mechanisms, including robust internal accountability structures and meaningful stakeholder engagement strategies, are essential for countering internal opacity, ensuring human oversight, and incorporating diverse perspectives into the governance process.

However, the effective implementation of these mechanisms faces significant hurdles, including organizational ignoring practices, institutional inertia, and a lack of clarity regarding accountability and liability across complex systems and multiple stakeholders. The discussion unequivocally demonstrated that achieving a balance between the efficiency gains sought through ADM and the imperative of procedural justice fundamentally requires a comprehensive, multi-layered governance approach. This approach must actively counter technical, organizational, and institutional blackboxing, ensuring transparency, accountability, and meaningful human involvement throughout the ADM lifecycle.

Ultimately, building and maintaining citizen trust in public sector ADM is intrinsically linked to the perceived fairness and accountability of these systems. Transparency mechanisms that reveal potential biases, coupled with governance structures that empower citizens with access to information and avenues for redress, are vital. Leveraging the inherent institutional trust in public sector organizations provides a foundation, but this trust must be actively reinforced through demonstrable commitment to transparent, accountable, and procedurally just ADM practices.

Based on these findings, policymakers and public sector practitioners should prioritize the development of integrated governance frameworks that combine technical safeguards with strong organizational and institutional controls. Key recommendations include mandating algorithmic impact assessments, establishing independent ethical review boards, implementing robust audit trails, and investing in training to enhance AI/ML competence among staff. Furthermore, embedding meaningful stakeholder engagement throughout the ADM lifecycle and establishing clear lines of accountability, including mechanisms for redress and legal adaptation, are critical steps towards building trust and ensuring that the pursuit of efficiency does not compromise procedural justice.

The findings underscore the need for ongoing research and practical application in the public sector. Future work should focus on developing and evaluating specific governance frameworks that explicitly address the trade-offs between efficiency and justice, exploring innovative approaches to stakeholder engagement in complex ADM contexts, and clarifying accountability structures across inter-organizational systems. Continued effort is required to ensure that the deployment of automated decision systems in the public sector serves the public good, upholding democratic values and fostering citizen trust.

## References

1. Sepideh Ebrahimi, Esraa Abdelhalim, Khaled Hassanein, Milena Head. (2024). Reducing the incidence of biased algorithmic decisions through feature importance transparency: an empirical study. *European Journal of Information Systems*.
2. Aleksandre Asatiani, Tuuli Hakkarainen, Kimmo Paaso, Esko Penttinen. (2023). Security by envelopment  a novel approach to data-security-oriented configuration of lightweight-automation systems. *European Journal of Information Systems*.
3. Tapani Rinta-Kahila, Ida Someh, Nicole Gillespie, Marta Indulska, Shirley Gregor. (2022). Algorithmic decision-making and system destructiveness: A case of automatic debt recovery. *European Journal of Information Systems*.
4. Constana M.R.P. Vaz-Patto, Fernando A.F. Ferreira, Kannan Govindan, Neuza C.M.Q.F. Ferreira. (2024). Rethinking urban quality of life: Unveiling causality links using cognitive mapping, neutrosophic logic and DEMATEL. *European Journal of Operational Research*.
5. Nizar Hoblos, MS Sandeep, Shan L Pan. (2023). Achieving stakeholder alignment in digital transformation: A frame transformation perspective. *Journal of Information Technology*.
6. Charlotta Kronblad, Anna Essn, Magnus Mhring. (2024). When Justice is Blind to Algorithms: Multilayered Blackboxing of Algorithmic Decision Making in the Public Sector.
7. Nripendra P. Rana, Sheshadri Chatterjee, Yogesh K. Dwivedi, Shahriar Akter. (2022). Understanding dark side of artificial intelligence (AI) integrated business analytics: assessing firms operational inefficiency and competitiveness. *European Journal of Information Systems*.
8. Rohit Nishant, Dirk Schneckenberg, MN Ravishankar. (2024). The formal rationality of artificial intelligence-based algorithms and the problem of bias. *Journal of Information Technology*.
9. He Huang, Peter Burgherr, Cathy Macharis. (2024). A collaborative group decision-support system: the survey based multi-actor multi-criteria analysis (MAMCA) software. *Journal of the Operational Research Society*.
10. Jeroen Baijens, Tim Huygh, Remko Helms. (2022). Establishing and theorising data analytics governance: a descriptive framework and a VSM-based view. *Journal of Business Analytics*.
11. (2021). Public policy and operations management. *Journal of Operations Management*.
12. Atta Addo. (2022). Orchestrating a digital platform ecosystem to address societal challenges: A robust action perspective. *Journal of Information Technology*.
13. Guang-Li Huang, Arkady Zaslavsky. (2024). Contextual knowledge graph approach to bias-reduced decision support systems. *Journal of Decision Systems*.
14. Leif Sundberg, Jonny Holmstrm. (2024). Fusing domain knowledge with machine learning: A public sector perspective. *Journal of Strategic Information Systems*.
15. Jiaqi Yang, Alireza Amrollahi, Mauricio Marrone. (2024). Harnessing the Potential of Artificial Intelligence: Affordances, Constraints, and Strategic Implications for Professional Services. *Journal of Strategic Information Systems*.
16. Alice H. Aubert, Judit Lienert. (2024). Operational Research for, with, and by citizens: An overview. *European Journal of Operational Research*.
17. Johan Magnusson, Dina Koutsikouri, Tero Pivrinta. (2020). Efficiency creep and shadow innovation: enacting ambidextrous IT Governance in the public sector. *European Journal of Information Systems*.
18. Alexander Stevens, Johannes De Smedt. (2024). Explainability in process outcome prediction: Guidelines to obtain interpretable and faithful models. *European Journal of Operational Research*.
19. Lisa Marie Giermindl, Franz Strich, Oliver Christ, Ulrich Leicht-Deobald, Abdullah Redzepi. (2022). The dark sides of people analytics: reviewing the perils for organisations and employees. *European Journal of Information Systems*.
20. Crispin Coombs, Donald Hislop, Stanimira K. Taneva, Sarah Barnard. (2020). The strategic impacts of Intelligent Automation for knowledge and service work: An interdisciplinary review. *Journal of Strategic Information Systems*.
21. Bjorn Binzer, Jennifer Kendziorra, Anne-Katrin Witte, Till J. Winkler. (2024). Trust in Public and Private Providers of Health Apps and Usage Intentions: A Sectoral Privacy Calculus and Control Perspective. *Bus Inf Syst Eng*.
22. Daniel Rudmark, Rikard Lindgren, Ulrike Schultze. (2024). Open data platforms: Design principles for embracing outlaw innovators. *Journal of Strategic Information Systems*.