<!--
Mission ID: cf4b4922-2bba-4fc8-8962-4fca5f77e48c
OpenRouter Models Configured:
  Light: openai/gpt-4o-mini
  Heavy: google/gemini-2.5-flash-preview
  Beast: google/gemini-2.5-flash-preview
Stats:
  Total Cost: $0.588152
  Total Native Tokens: 3421972
  Total Web Searches: 0
Generated via: Streamlit UI
-->

# Governing Automated Public Sector Decisions: Balancing Efficiency, Justice, and Trust

# 1. Introduction

The increasing integration of automated decision systems (ADS) into public sector operations marks a significant shift in how governmental functions are performed and services are delivered. While these systems offer the potential for enhanced efficiency, speed, and consistency in decision-making, their deployment also introduces complex challenges, particularly concerning procedural justice. The inherent tension between the pursuit of algorithmic efficiency and the imperative to ensure fair, equitable, and transparent processes is a critical issue that demands careful consideration. This tension arises because algorithmic processes, driven by data and code, can operate opaquely, embed or amplify existing societal biases, and challenge traditional notions of human accountability and discretion.

Addressing this tension necessitates the development and implementation of robust governance mechanisms. Effective governance is crucial not only for mitigating the risks associated with algorithmic bias and opacity but also for ensuring that the use of ADS in the public sector aligns with democratic values and maintains citizen trust. Without appropriate oversight and accountability structures, there is a risk that efficiency gains could come at the cost of fundamental rights, fairness, and public confidence in governmental institutions.

This report explores the academic literature surrounding automated decision systems in the public sector, focusing on the governance mechanisms proposed to navigate the tension between algorithmic efficiency and procedural justice. It examines the sources and nature of algorithmic bias and strategies for its mitigation. Furthermore, it delves into proposed accountability frameworks for public sector ADS and the challenges associated with their implementation, drawing on case studies and empirical evidence. Finally, the report considers the vital role of citizen engagement in the governance process and explores methods and technologies for fostering public participation. By synthesizing insights from these areas, this report aims to contribute to a better understanding of how public sector organizations can responsibly deploy ADS while upholding principles of justice, accountability, and trust.

The following section will delve into a review of the relevant literature, beginning with an examination of algorithmic bias and its governance.

# 2. Literature Review

The increasing deployment of automated decision systems (ADS) within the public sector necessitates a comprehensive understanding of governance mechanisms capable of navigating the inherent tension between algorithmic efficiency and procedural justice. This tension arises from the potential for ADS to introduce or amplify biases, operate opaquely, and challenge traditional accountability structures, thereby impacting citizen trust and democratic accountability. Academic literature explores various facets of this challenge, proposing governance models and highlighting critical issues such as algorithmic bias, the need for robust accountability frameworks, and the role of citizen engagement. This section synthesizes key findings from this literature, providing an overview of proposed governance models, their approaches to procedural justice, and existing evidence regarding their effectiveness.

A significant body of literature focuses on algorithmic bias, which represents a substantial threat to procedural justice in public sector ADS. Bias can manifest through the use of sensitive variables in decision-making [1], potentially leading to direct discrimination. More subtly, indirect discrimination can arise even when algorithms use seemingly neutral variables, if these variables reflect underlying societal inequities present in the training data [1]. Transparency is often proposed as a mitigation strategy, but its effectiveness is nuanced. Feature Importance (FI) transparency, which reveals the variables used and their contribution, can increase awareness of direct discrimination, but is less effective for indirect bias [1]. Addressing indirect discrimination may require combining FI with Algorithmic Demographic Information (ADI) to reveal variable distribution across demographic groups [1]. However, revealing direct discrimination through transparency can paradoxically decrease user trust, mediated by reduced perceptions of fairness [1]. This highlights the need for carefully designed transparency mechanisms that balance complexity and simplicity to avoid overwhelming users and eroding trust [1]. Ethical challenges in AI-based decision-making extend beyond bias to encompass objectivity, privacy, transparency, accountability, and trustworthiness [15].

Beyond addressing algorithmic bias, the implementation of effective accountability frameworks is crucial for ensuring responsible ADS use in the public sector. The Australian "Robodebt" program serves as a stark case study illustrating the socially destructive consequences of governmental ADM when governance fails [4]. This program, designed for efficiency in debt recovery, resulted in invalid debt estimates, severe citizen distress, and ultimately, legal action due to a lack of robust governance infrastructures [4]. The case highlights issues of algorithmic opacity and difficulty in establishing accountability and trust [4]. Mechanisms contributing to such destructive outcomes include managerial vision driven by ideology ("directing change with limited vision"), minimizing human agency ("limiting sociotechnical agency"), ignoring negative signals ("dismissing destructive consequences"), and the eventual societal response triggering delegitimization [4]. The Robodebt case underscores critical deficiencies in existing legal frameworks and oversight mechanisms, suggesting that deploying organizations must proactively adopt best-practice governance frameworks, including human oversight, risk identification/mitigation, accuracy checks, and independent complaint investigation processes [4][4]. Academic literature proposes various models and mechanisms for governing data analytics and AI, which can inform the design of ADS governance in the public sector. One descriptive framework for data analytics governance proposes a typology of structural, process, and relational mechanisms [10]. Structural mechanisms relate to organizational design, roles, responsibilities, and coordination structures [10]. Process mechanisms involve formal procedures for monitoring, evaluation, and development [10]. Relational mechanisms focus on fostering shared perceptions, collaboration, and knowledge transfer among stakeholders [10]. While this framework primarily addresses value creation and efficiency in business contexts, it acknowledges the ethical use of data as a challenge [10]. Governance challenges specific to public sector ADS include a lack of clear governance mechanisms for strategic AI projects, difficulties in strategic alignment and integration, legal and ethical challenges (particularly regarding data privacy), balancing automation with human expertise and trust, competence gaps among staff, and the challenge of translating AI policies to operational levels [14]. Successful governance in this context requires aligning AI development with the public service ethos and strategies to maintain public trust [14]. Beyond technical opacity, "organizational ignoring practices" by public institutions can lead to "multilayered blackboxing" of ADM systems, contributing to social and legal injustice [7]. This involves deliberate non-confrontation with ADM risk, obscuring information, and denying the need to adapt institutional frameworks, leading to diffused accountability [7]. To counteract this, a framework has been proposed addressing different layers of blackboxing: technological (explainability/transparency), intra-organizational (engagement/responsibility), extra-organizational (access/power balance), and institutional (recourse and restitution) [7][7]. This framework emphasizes actions from technology developers, public agencies, legislators, and legal professionals, including ensuring open code and documentation, public agencies taking accountability for explaining ADM use and risks, providing information access to external parties, empowering legal institutions to assess algorithm legality, and potentially creating an ombudsperson for algorithmic justice [7][7]. This multilayered approach recognizes that addressing algorithmic injustice requires institutional adaptation and legal reform, not just technical fixes [7]. The concept of governance in AI-integrated systems is critical for successful implementation and helps narrow the gap between accountabilities and ethics [8]. Lack of governance can lead to acquiring flawed data and contribute to system opacity, resulting in suboptimal decisions and increased risks [8]. Responsible and accountable decision-making in intelligent automation is paramount, especially in sensitive areas [17]. While automation might be free from human emotion, transparency in decision-making is needed for healthy machine-user interactions [17]. Determining responsibility for negative consequences is challenging given multi-stakeholder involvement, with a consensus view suggesting ultimate responsibility rests with human stakeholders [17]. Research gaps exist in designing socially acceptable values into intelligent automation and testing this, developing policies for integrating human judgment, and defining the balance of control between humans and automation [17][17]. Trust is a mediating factor in system acceptance, its significance magnified by the shift in decision-making control [17].

Finally, the literature highlights the importance of citizen engagement in the governance of public sector ADS. Transparency itself is a well-established construct in public sector theory, linked to improving government quality, facilitated by internet technologies [13]. It involves both information availability and the flow of information, with citizens as a primary audience [13]. While transparency can facilitate participation and improve financial management, it has not always proven effective in engendering trust in government [13]. Citizen engagement mechanisms can provide avenues for public input and oversight, contributing to democratic accountability and potentially enhancing trust by making governance processes more inclusive and responsive.

This review of the literature highlights that effective governance of public sector ADS requires a multifaceted approach that goes beyond technical solutions to address organizational practices, legal and institutional frameworks, and the complex interplay between transparency, trust, and accountability. The following sections will delve deeper into specific areas, beginning with algorithmic bias and its governance.

# 3. Algorithmic Bias and Governance

Algorithmic bias presents a significant challenge to achieving procedural justice in public sector automated decision systems (ADS). This bias can manifest in various ways, impacting the fairness and equity of algorithmic outcomes [1]. A primary source of bias lies within the datasets used for training algorithms. Biased, mislabelled, unbalanced, or non-representative data can lead to models that perpetuate or amplify existing societal inequalities and historical discrimination [1][7]. Methodological issues during model development, such as overgeneralization or overfitting, can also introduce bias [1]. Furthermore, the inclusion of protected variables like sex or race, or their proxies, either explicitly coded or learned from biased historical decisions, constitutes a direct source of discrimination [1].

Beyond data and explicit coding, algorithms can also produce indirect discrimination. This occurs when decisions based on seemingly neutral variables unintentionally disadvantage a protected group due to underlying biases in the data or inadvertent modelling procedures [1]. An important, and potentially novel, source of bias in machine learning models is gender-blind feature selection, where excluding gender as a feature can paradoxically lead to women being more detrimentally impacted by the model's reliance on other features [6]. The fundamental difference between algorithmic formal rationality (quantitative calculations, rigid rules) and human substantive rationality (contextual understanding, value judgments) in interpreting complex, real-world data can also lead to biased outcomes, even with seemingly unbiased data [9]. AI struggles to interpret contextually embedded values, norms, and moral judgments inherent in human communication and datasets, creating blind spots for identifying biases related to equal treatment [9][9]. Examples of algorithmic bias include discriminatory hiring tools, racist labeling in image recognition, and biased student grading systems [9].

Governance strategies are crucial for mitigating algorithmic bias and its detrimental effects on procedural justice. Algorithmic transparency is a prominent approach, aiming to reveal potential discrimination that might otherwise remain hidden [1]. Different types of transparency are needed to address different forms of bias [1]. Feature Importance (FI) transparency, which illustrates the nature and weights of features used, can help identify areas of bias [1]. Empirical studies show that providing FI can increase users' moral awareness and reduce the adoption of directly discriminatory recommendations [1]. However, FI alone is often insufficient for detecting indirect discrimination [1][f96d7c5c].

To address indirect discrimination, demographic-based transparency is suggested [1]. Algorithmic Demographic Information (ADI), which shows the distribution of variables across demographic groups, can help users identify if a seemingly neutral variable adversely affects a specific group [1][f96d7c5c]. Combining FI with ADI has been shown to increase users' moral awareness and reduce the likelihood of approving indirectly discriminatory recommendations [1][f96d7c5c]. Providing fine-grained demographic transparency measures is considered an asset for accountable use of algorithmic systems [1].

However, the relationship between transparency and trust is complex. While transparency can trigger moral awareness by highlighting ethical conflicts, revealing direct discrimination can negatively impact users' perceived fairness and consequently reduce trust in the system [1][f96d7c5c]. This underscores the need for careful design of transparency mechanisms to avoid cognitive overload and unintended negative impacts on trust [1].

Beyond transparency, other governance approaches address bias at different stages. Addressing biased training datasets is critical, with proposed methods including down sampling or up sampling data to rebalance demographic representation [6]. Gender-aware hyperparameter tuning and, in certain legal contexts, probabilistic gender proxy modeling are also explored as techniques to reduce discrimination [6]. Acknowledging that purely technical solutions are insufficient, sociotechnical perspectives emphasize combining machine learning with human supervision trained on bias and discrimination issues [9]. Defining clear roles and responsibilities for AI and human intervention based on the complexity and value-laden nature of the task is also proposed [9].

Addressing algorithmic bias also requires proactive measures from deploying organizations, including adopting best-practice frameworks for ethical ADM use [4]. This involves appropriate human oversight, systematic risk identification and mitigation, checking algorithmic accuracy and robustness, and ensuring effective independent investigation processes for complaints [4]. The ability to collect and use protected attribute data, while raising privacy concerns, is argued by some to be crucial for organizations to effectively test for and measure discrimination to implement mitigation strategies [6]. This highlights potential inconsistencies between existing anti-discrimination laws and the requirements for effectively governing algorithmic bias [6].

Organizational practices of "ignoring" can contribute to the problem by obscuring information about ADM agency and consequences, preventing scrutiny and accountability [7]. Counteracting this requires addressing not only technological opacity but also intra-organizational, extra-organizational, and institutional practices that enable layered blackboxing [7]. This includes holding public agencies accountable for explaining ADM use, providing access to information, and empowering legal institutions to assess algorithm legality [7].

In summary, algorithmic bias is a multifaceted issue arising from data, algorithms, and socio-technical practices. Governance strategies to mitigate bias involve enhancing transparency through mechanisms like FI and ADI, employing technical techniques to address data imbalances, incorporating human oversight, and reforming organizational and institutional practices to ensure accountability and access to information. However, the effectiveness of these strategies can be nuanced, requiring careful consideration of their impact on perceived fairness and trust.

The following sections will delve deeper into specific accountability frameworks for public sector ADS and the challenges in their implementation.

# 4. Sources and Nature of Algorithmic Bias in Public Sector ADS

Algorithmic bias in public sector automated decision systems (ADS) is a socio-technical phenomenon with diverse sources and significant implications for procedural justice. This bias can originate from the data used to train algorithms, methodological choices during model development, and the explicit or implicit inclusion of protected characteristics [1]. Biased, mislabelled, unbalanced, or non-representative datasets are a primary source, causing algorithms to inherit and perpetuate existing societal inequalities and historical discrimination [1][7]. Methodological issues, such as overgeneralisation and overfitting, can also contribute to biased outcomes [1]. Explicitly coding protected characteristics like sex or race into an algorithm, or the algorithm learning from historical data where these characteristics were influential, can lead to direct discrimination [1].

Beyond direct discrimination, algorithms can also produce indirect discrimination. This occurs when decisions are based on seemingly neutral variables that, due to underlying correlations with protected characteristics within biased or non-representative data, unintentionally disadvantage a specific group [1]. For instance, using geographical location as a primary factor in school placement can lead to unfair outcomes if certain demographics are concentrated in areas with limited school access, as illustrated by a case study in Gothenburg [7][7]. Similarly, predicting medical appointment no-shows based on variables correlated with race, due to socioeconomic factors, can result in longer waiting times for minority groups [5]. Another source of bias, particularly in machine learning models, is gender-blind feature selection. Excluding gender as an explicit feature can paradoxically lead to the model relying on other features that disproportionately impact women, demonstrating that simply removing protected attributes does not guarantee fairness [6].

The nature of algorithmic bias also stems from the inherent limitations of formal rationality in AI when interpreting complex, value-laden real-world situations. AI struggles to grasp the nuanced hermeneutic structure of natural language and contextual ethical considerations, often relying on average judgments rather than specific, verifiable answers needed for moral challenges [9]. This gap between the data used for training (often sanitized) and the complexity of real-life communication can generate and perpetuate judgmental biases [9].

The manifestation of algorithmic bias directly impacts procedural fairness, which requires decision-making processes to be accurate, unbiased, correctable, and ethical [1]. Biases in algorithmic recommendations lead to an unequal distribution of benefits or burdens among different groups, resulting in discrimination and negatively affecting users' perceptions of fairness [1]. Discriminatory decisions, whether direct or indirect, are typically perceived as unfair and can provoke ethical outrage [1]. The use of ADM systems can exacerbate existing socioeconomic inequalities, solidify historical biases, and reinforce discrimination based on protected characteristics [7]. New forms of injustice can also emerge, such as the personalization of services limiting options for certain groups, leading to marginalization [7]. Algorithmic injustice can encompass not only the maldistribution of resources but also misrecognition (reinforcing status inequalities) and misrepresentation (unequal access to democratic institutions and means of redress) [7].

Beyond the technical and data-related sources, organizational and institutional practices can significantly contribute to algorithmic bias and injustice in the public sector [7]. Public agencies and legal institutions may fail to adequately respond to the detrimental consequences of ADM, engaging in "organizational ignoring practices" that create "multilayered blackboxing" [7]. This involves avoiding confrontation with risks, obscuring information from external stakeholders, and denying the need to adapt institutional frameworks, hindering the identification and correction of errors and perpetuating social and legal injustice [7]. The Gothenburg school placement case illustrates how a public administration's avoidance of internal warnings, obscuring information from the public, and insistence on treating systematic errors as individual issues, combined with the court system's failure to adapt legal procedures, led to thousands of uncorrected erroneous decisions [7][7].

In summary, the sources of algorithmic bias in public sector ADS are multifaceted, arising from data characteristics, algorithmic design choices, the inherent limitations of AI's formal rationality, and organizational and institutional practices. These biases manifest as direct and indirect discrimination, impacting the equitable distribution of resources and services, and undermining procedural justice and fairness perceptions. The following section will examine governance mechanisms proposed to mitigate these forms of algorithmic bias.

# 5. Governance Mechanisms for Mitigating Algorithmic Bias

Governance mechanisms aimed at mitigating algorithmic bias in public sector automated decision systems (ADS) are crucial for upholding procedural justice and maintaining public trust. These mechanisms span technical, organizational, and institutional levels, reflecting the multifaceted nature of algorithmic bias.

A prominent technical approach is algorithmic transparency, which seeks to reveal potential discrimination [1]. As discussed previously, different forms of transparency exist, such as Feature Importance (FI) transparency, which illustrates the variables used and their weights, and Algorithmic Demographic Information (ADI), which shows variable distribution across demographic groups [1]. While FI can increase awareness of direct discrimination, ADI is particularly useful for identifying if seemingly neutral variables disproportionately affect specific groups, thereby addressing indirect bias [1]. Combining FI with ADI has been shown to enhance moral awareness and reduce the approval of indirectly discriminatory recommendations, and providing fine-grained demographic transparency measures is considered beneficial for accountable system use [1]. However, the impact of transparency on trust is complex; while increasing awareness of ethical issues, revealing direct discrimination can decrease perceived fairness and trust [1]. This highlights the need for careful design of transparency mechanisms to avoid overwhelming users or causing unintended negative consequences for trust [1].

Beyond transparency, technical interventions can address bias at the data level. Methods include down sampling or up sampling training data to rebalance demographic representation [6]. Gender-aware hyperparameter tuning and, where legally permissible, probabilistic gender proxy modeling are explored as techniques to reduce discrimination [6]. Some argue that allowing the responsible collection and use of protected attribute data is necessary for organizations to effectively measure and test for discrimination to implement mitigation strategies [6]. This perspective highlights potential inconsistencies between existing anti-discrimination laws and the requirements for effectively governing algorithmic bias [6].

Organizational and institutional governance mechanisms are also critical. Proactive adoption of best-practice frameworks for ethical ADM use by deploying organizations is essential [4]. This involves ensuring appropriate human oversight, systematic risk identification and mitigation, verifying algorithmic accuracy and robustness, and establishing effective independent investigation processes for complaints [4]. Sociotechnical perspectives emphasize combining machine learning with human supervision trained on bias and discrimination issues, defining clear roles and responsibilities based on task complexity and value-laden nature [9].

Counteracting "organizational ignoring practices" that lead to "multilayered blackboxing" requires addressing not only technological opacity but also intra-organizational, extra-organizational, and institutional practices [7]. This includes holding public agencies accountable for explaining ADM use and risks, providing access to information about the system to external parties upon request (e.g., via Freedom of Information Act), and empowering legal institutions to assess algorithm legality and hold actors accountable [7]. Proposed institutional mechanisms include adapting legal frameworks for fair proceedings, potentially shifting the burden of proof when public agencies use ADM, and considering appellate processes for systematic reversal of bulk decisions [7]. The creation of an algorithmic justice ombudsperson has also been suggested to help vulnerable groups safeguard their rights and serve as a counterweight to the power imbalance with ADM-using organizations [7]. Empirical evidence on the effectiveness and challenges of these specific institutional and organizational mechanisms in practice requires further investigation, though case studies like Robodebt highlight the severe consequences of their absence [4].

Having discussed governance mechanisms for mitigating algorithmic bias, the following section will explore broader accountability frameworks for public sector ADS.

# 6. Accountability Frameworks

Accountability for automated decision systems (ADS) in the public sector is a critical area of research and practice, necessitating the development and implementation of robust frameworks. These frameworks are essential for ensuring that ADS operate in alignment with democratic values, uphold procedural justice, and maintain citizen trust. They extend beyond merely technical considerations to encompass organizational practices, legal structures, and institutional mechanisms. As highlighted by the Robodebt case, a lack of effective governance and oversight can lead to socially destructive outcomes, underscoring the need for proactive measures from deploying organizations [4]. This includes adopting best-practice frameworks that ensure appropriate human oversight, systematic risk identification and mitigation (such as impact assessments and ethical review boards), checking algorithmic accuracy and robustness, and establishing independent investigation processes for complaints [4].

One proposed legal framework specifically addresses the "multilayered blackboxing" that can occur in public administration ADM, contributing to social and legal injustice [7]. This framework seeks to counteract "organizational ignoring practices" by proposing principles and actions across different layers of opacity [7]. At the technological level, principles of explainability and transparency are emphasized, recommending that technology developers provide open code and documentation and that public agencies ensure these are included in procurement agreements to enable critical scrutiny [7]. Intra-organizational blackboxing, which arises from an organization avoiding confrontation with ADM risks, is addressed through principles of engagement and responsibility. This requires public agencies to be accountable at both individual and organizational levels for explaining ADM use, risks, and consequences, monitoring regulatory compliance, and verifying that code aligns with regulations [7].

Extra-organizational blackboxing, involving the intentional obscuring of information from external stakeholders, is countered by principles of access and power balance. Recommended actions include public agencies issuing declarations of ADM use, providing code and system information to service recipients, media, and external parties upon request (e.g., via Freedom of Information Act), and being obliged to prove the legality of implemented ADM systems in legal contestations [7]. This empowers external actors and service recipients to investigate and take action [7]. Finally, institutional blackboxing, stemming from the inability of legal frameworks to adapt to ADM, is addressed through the principles of recourse and restitution. This involves legislative actors updating laws to acknowledge the systematic nature of ADM decisions, allowing for systematic appeals and legality assessments of code, requiring systematic review and correction of erroneous outcomes, training legal professionals to assess ADM systems, and shifting the burden of proof of ADM legality from the plaintiff to the public agency [7]. The framework also suggests the creation of an ombudsperson for algorithmic justice to support vulnerable groups in safeguarding their rights and serving as a counterweight to power imbalances [7].

Implementing these frameworks faces significant challenges. The Robodebt case demonstrated how political ideology and economic imperatives could lead to a limited managerial vision that prioritizes cost savings over service delivery and risk mitigation [4]. This can result in limiting human agency, dismissing destructive effects, and allowing socially harmful systems to persist despite negative consequences [4]. The case also highlighted a lack of sufficient legal infrastructures and the potential for government organizations to be less responsive to calls for change from regulatory bodies compared to the private sector [4]. Furthermore, the "robust action" approach to government orchestration of digital platform ecosystems, while promoting flexibility, can conflict with the need for transparency and accountability in the public sector, as it may involve cultivating ambiguity to keep options open [11].

Additional challenges include the lack of clarity on how accountability is practically realized given the multiple stakeholders involved in Intelligent Automation applications [17]. While there is a consensus that human stakeholders are likely to be held accountable, the legal liability might extend to machine designers, manufacturers, implementers, and users [17]. Technology firms often state their products support decisions but place ultimate responsibility on the human operator [17]. Research is needed to understand how organizations determine responsibility for the consequences of ADM investments and how contextual factors influence the balance of control between human judgment and automation [17].

The effectiveness of accountability frameworks is also contingent on the capacity and competency within government organizations to manage ADM programs responsibly, including investing in both technical and human resources with sufficient domain sensitivity [4]. The tension between algorithmic efficiency and procedural justice is implicitly addressed by frameworks that propose multi-layered mechanisms to ensure transparency, accountability, access to information, and effective recourse, although this may impact the speed or simplicity of algorithmic deployment [7].

The following sections will delve deeper into specific proposed accountability frameworks for public sector ADS and the challenges encountered in their implementation, drawing on case studies and empirical evidence.

# 7. Proposed Accountability Frameworks for Public Sector ADS

This section provides an overview of proposed accountability frameworks specifically designed for public sector automated decision systems (ADS), highlighting their objectives and mechanisms. Effective accountability for ADS in the public sector requires frameworks that address not only technical aspects but also organizational and institutional dimensions [4].

One significant proposed legal framework addresses the issue of "multilayered blackboxing" in public administration ADM, which can contribute to social and legal injustice [7]. This framework aims to counteract "organizational ignoring practices" by proposing principles and actions across different layers of opacity [7]. At the technological layer, the principles of explainability and transparency are central. Recommended actions include technology developers providing open code and documentation, and public agencies ensuring these are included in procurement agreements, preventing code from being treated as a trade secret [7]. This facilitates external scrutiny of the algorithmic logic. Intra-organizational blackboxing, which stems from an organization's avoidance of confronting ADM risks, is addressed through the principles of engagement and responsibility [7]. This requires public agencies to be accountable for explaining the use, risks, and consequences of ADM systems, monitoring regulatory compliance and risks, and verifying that the implemented code and processes align with applicable regulations, including making public translations of regulations into code [7]. Extra-organizational blackboxing, involving the intentional obscuring of information from external stakeholders, is countered by the principles of access and power balance [7]. Proposed actions include public agencies issuing declarations of ADM system use, providing code and system information to service recipients, media, and other external parties upon request, consistent with Freedom of Information Act or equivalent legislation [7]. Public agencies should also provide overviews summarizing algorithmic functions, data used, code, and regulatory compliance, and be obliged to prove the legality of their ADM systems in legal challenges and court hearings [7]. These measures aim to empower external actors and affected individuals to investigate and contest ADM outcomes. Finally, institutional blackboxing, arising from the inability of existing legal frameworks to adequately address ADM, is tackled through the principles of recourse and restitution [7]. This involves legislative actors updating laws to recognize the systematic nature of ADM decisions, potentially allowing for systematic appeals and legality assessments of code [7]. It also requires systematic review and correction of erroneous ADM outcomes, providing courts and judges with resources and training to assess ADM systems, and establishing legal procedures that shift the burden of proving ADM legality from the plaintiff to the public agency [7]. The framework also suggests creating an ombudsperson for algorithmic justice to support the public, particularly vulnerable groups, in contesting ADM outcomes and serving as a counterweight to power imbalances with ADM-deploying organizations [7]. This multi-layered approach recognizes that effective accountability requires institutional adaptation and legal reform alongside technical and organizational changes [7].

Beyond this specific framework, other best-practice frameworks for ethical ADM use by deploying organizations are also proposed, particularly in light of cases like Robodebt [4]. These frameworks should include ensuring appropriate human oversight, identifying, mitigating, and monitoring risks to stakeholders through mechanisms like impact assessments and ethical review boards, checking the accuracy and robustness of algorithmic outcomes before deployment, and establishing effective independent investigation processes in response to complaints [4]. Successful implementation of ADM necessitates that governments develop sufficient capacity and competency to run such programs responsibly, investing in both technical and human resources [4].

These proposed frameworks implicitly address the tension between algorithmic efficiency and procedural justice by introducing mechanisms designed to ensure transparency, accountability, access to information, and effective recourse, even if this adds complexity or time to decision processes compared to purely automated approaches [7].

Having examined various proposed accountability frameworks, the following section will discuss the challenges encountered in their implementation, drawing on case studies and empirical evidence.

# 8. Challenges in Implementing ADS Accountability Frameworks: Case Studies and Empirical Evidence

Implementing accountability frameworks for automated decision systems (ADS) in the public sector presents significant challenges, as evidenced by real-world case studies and empirical observations. These difficulties extend beyond technical complexities to encompass organizational practices, institutional limitations, and socio-political factors, often leading to detrimental outcomes for citizens and undermining democratic accountability.

A prominent example illustrating these challenges is the Australian Robodebt program, an automated welfare debt recovery system. This case demonstrates how a narrow managerial vision focused on cost savings and revenue generation, coupled with a welfare-critical ideology, led to a program design with limited human agency and technological limitations [4]. The system's inability to accurately process complex welfare payment data, combined with the removal of human oversight and a reversal of the onus of proof onto citizens, resulted in thousands of erroneous debt notices and severe distress [4]. Despite mounting criticism from citizens, internal staff, and oversight bodies like the Commonwealth Ombudsman, the program persisted due to an escalation of commitment from top management who dismissed the destructive effects in favor of perceived economic benefits [4]. The Robodebt case highlights critical deficiencies in existing legal frameworks and oversight mechanisms for government ADS use, as regulatory inquiries failed to address fundamental issues like the legality of the debt-averaging method or the reversal of the burden of proof [4]. The system was ultimately halted only after citizens initiated legal action, which forced governance activation and led to a court ruling declaring the debt-averaging method illegal [4]. This suggests that government organizations may be less responsive to regulatory and oversight bodies compared to private sector entities, and that legal action can serve as a critical, albeit often delayed, mechanism for enforcing accountability when other avenues fail [4]. The lengthy period between initial concerns and the final court ruling also points to a lack of sufficient legal infrastructure capable of addressing the systematic errors produced by ADM [4].

Further empirical evidence underscores how organizational practices can impede accountability. The concept of "organizational ignoring practices" contributes to "multilayered blackboxing" of ADM systems in the public sector, leading to social and legal injustice [7]. These practices include public agencies *avoiding* confrontation with the risks and consequences of ADM, leading to intra-organizational blackboxing and the normalization of non-compliant practices [7]. They also involve *obscuring* information about the ADM system and its implementation from external stakeholders, creating extra-organizational blackboxing that shields the organization from criticism and prevents external scrutiny [7]. Finally, public institutions may *deny* the need to adapt their frameworks to address ADM, resulting in institutional blackboxing that hinders legal redress and accountability [7]. These practices reinforce each other, diffusing accountability and maintaining a status quo detrimental to justice outcomes, even with technologically simple algorithms in contexts with high trust and transparency [7]. The Gothenburg school placement case, where a rule-based algorithm produced thousands of erroneous decisions violating national law, illustrates these ignoring practices in action [7]. The public administration avoided internal warnings, obscured information from the public, and insisted systematic errors were individual issues, while the court system failed to adapt legal procedures, leading to widespread uncorrected errors [7].

The limited empirical research on how accountability is realized in practice for Intelligent Automation (IA) systems, particularly in public sector contexts like service delivery for vulnerable people, highlights a significant knowledge gap [17]. While clear lines of governance and human accountability are recommended in such sensitive areas, there is a need for further research into how organizations actually devise policies and procedures for combining human judgment with IA and determining the balance of control [17]. The multi-stakeholder nature of ADM applications also complicates the practical realization of accountability and the management of legal liability, with questions remaining about how organizations decide responsibility for consequences [17]. The academic literature identifies significant research gaps concerning the practical implementation of governance arrangements and structures for IA decision-making in various contexts, including those relevant to the public sector [17].

These case studies and empirical observations demonstrate that implementing accountability frameworks requires addressing not only the technical design of ADM systems but also the organizational culture, managerial vision, legal infrastructure, and institutional capacity to respond to errors and negative consequences effectively. Challenges include overcoming organizational resistance to acknowledging flaws, ensuring sufficient transparency and access to information for external scrutiny, and adapting legal and regulatory frameworks to the unique challenges posed by automated decision-making.

Having examined the challenges in implementing accountability frameworks, the following section will explore various methods and technologies for citizen engagement in the governance of public sector ADS.

# 9. Citizen Engagement in Governance

Citizen engagement plays a crucial role in the governance of automated decision systems (ADS) within the public sector, offering mechanisms to involve the public in decision-making processes and potentially enhance trust and accountability. Citizen participation in public decision-making, including areas where automated systems are applied, is not a new concept, with examples dating back to the 1970s involving multidirectional information flow and community application of methods like Operational Research (OR) [16]. Participation is often driven by a moral rationale grounded in democratic ideals and a commitment to incorporating diverse perspectives, including those of minorities [16]. It can aim to increase the breadth of involvement (engaging many citizens) or the depth (fostering deep understanding among representatives), or both [16].

Progress in Information and Communication Technology (ICT) has significantly enabled broader citizen participation, allowing many individuals without formal political mandates to engage through digital tools [16]. Examples of digital tools include online platforms for decision analysis, digitized frameworks, unified toolkits for community decisions, e-participation and e-negotiation systems, and e-voting procedures for participatory budget allocations [16]. Data science also offers new avenues for engagement by enabling citizens to act as data providers or users, with open data initiatives improving transparency between citizens and public administrations [16]. This technological evolution has moved from "teledemocracy" to "e-democracy" and "e-governance," with the latter encompassing managing institutions with electronic tools and online service delivery [16]. M-governance, utilizing mobile technology, further expands these possibilities [16].

However, technology-enabled engagement faces limitations. The digital divide, encompassing access to technology and digital literacy, can marginalize segments of the population [16]. Digital tools may still require facilitators to guide participation and ensure intended use [16]. Challenges also exist in identifying and attracting all relevant stakeholders in their diversity and ensuring inclusive processes, particularly at the macro (nationwide) scale [16]. A critical aspect is the distribution of power and the degree to which citizens influence problem identification, formulation, and solution [16]. Community OR, for instance, explicitly aims at empowering communities [16].

Citizen engagement can significantly influence perceptions of fairness and trust in ADS. As discussed in previous sections, transparency is crucial for users' perceived fairness and trust in algorithmic systems, sometimes outweighing actual performance [1]. Procedural fairness, emphasizing the justice of the decision-making process itself, including accuracy, lack of bias, and correctability, influences attitudes towards institutions and authorities [1]. Providing transparency that highlights potential biases, particularly indirect discrimination through demographic information, can increase awareness of the moral implications of algorithmic decisions and impact perceptions of fairness [1]. Empirical findings suggest that transparency revealing direct discrimination can increase users' moral awareness while decreasing trust in a biased system [1]. This highlights that while transparency is essential for revealing potential unfairness, the reaction to such revelations can be complex, potentially reducing trust in the system itself while increasing awareness of ethical issues [1]. Process-based trust, rooted in the transparency and explainability of algorithms, is important for user confidence [1].

Empowerment is a key factor driving citizen engagement, encompassing competence, meaning, impact, and self-determination in the public domain [3]. Technology can facilitate empowerment, although lack of skills or unsupportive social structures can diminish this [3]. Building and maintaining trust is fundamental for motivating citizen participation and commitment to outcomes, requiring attention to diversity, equity, inclusion, and social justice [16]. Trust in the organization or sponsor is necessary for citizens to share information, especially in online processes where data protection and privacy are sensitive issues [16]. Voluntary consent and robust data protection measures are essential [16].

Strategies to enhance citizen engagement in ADS governance can draw on various methods. Face-to-face methods used in OR, such as facilitated workshops, citizen's juries, and focus groups, can foster deep deliberation among smaller groups [16]. Digital tools, including online surveys, e-negotiation platforms, and e-voting, can facilitate broader participation [16]. Deliberative approaches, like Citizens' Juries, which involve providing participants with balanced information and expert perspectives followed by deliberation, can be used to engage the public in shaping data governance practices for technologies [2]. Such methods align with Responsible Research and Innovation principles by involving citizens in the development and governance of technology [2]. When engaging large numbers of participants, methods for aggregating diverse inputs are necessary, ranging from considering different voices in qualitative approaches to mathematical aggregation in quantitative methods [16].

In public sector ADM governance, public agencies have a responsibility to articulate the nature of ADM use, risks, and consequences [7]. Providing information about algorithms, including overviews of their functions and making public translations of regulations into code, aims to empower citizens and external actors to investigate and take action [7]. Such transparency is a prerequisite for meaningful engagement.

Evaluating citizen participation is challenging and context-specific, often requiring mixed methods and mid- to long-term involvement, ideally based on stakeholders' perceptions of outcomes [16]. Future research is needed to empirically investigate digital OR methods and tools, explore broad participation suitability at the societal scale, and systematically report and evaluate participatory processes [16].

The following section will delve deeper into specific methods and technologies used for citizen engagement in ADS governance.

# 10. Methods and Technologies for Citizen Engagement in ADS Governance

Citizen engagement in the governance of automated decision systems (ADS) within the public sector can be facilitated through a variety of methods and technologies, aiming to integrate public perspectives into the design, deployment, and oversight of these systems. Historically, citizen participation in public decision-making, including areas now utilizing automated systems, has employed methods such as citizen forums and participatory workshops [16]. These traditional approaches often involve face-to-face interaction and facilitated discussions to gather detailed input from smaller groups [16].

Technological advancements, particularly in Information and Communication Technology (ICT), have significantly expanded the potential for citizen engagement, enabling broader participation through digital tools [16]. Digital platforms can support various forms of engagement, from simple information provision and collection via websites and emails to more interactive formats like chats, forums, and specifically designed interfaces for e-participation and e-negotiation [16]. Examples of digital tools used in public decision-making contexts include platforms for Multi-Criteria Decision Analysis (MCDA), digitized frameworks like MauriOmeter for value-aligned decisions, and e-voting procedures for participatory budget allocations [16]. The emergence of e-governance and m-governance further facilitates online service delivery and interaction between governments and citizens [16].

While specific examples of digital engagement directly applied to public sector ADS governance are less prevalent in the provided literature, case studies focusing on broader public decision-making or technology-enabled community initiatives offer transferable insights. For instance, studies exploring technology as a stimulus for participation in local energy communities involving smart meters and renewable installations demonstrate how technologies designed to foster empowerment and align with values can drive engagement [3]. Empowerment in this context relates to individuals feeling competent, perceiving value and impact in their participation, and experiencing self-determination [3]. Furthermore, some publications within the digital platforms literature specifically mention "governance" and "political participation" as developmental topics, with examples including SMS systems for political projects and participation websites [12]. Although these examples may not directly involve complex ADS, they illustrate the application of digital tools for political and governance-related participation. The use of digital tools in related areas like participatory budgeting and community decision-making [16] suggests their potential for facilitating engagement in the context of automated systems.

Data science also presents opportunities for engagement by enabling citizens to contribute data or utilize publicly available datasets [16]. Open data initiatives, for instance, enhance transparency and communication by allowing citizens to reuse governmental data for governance-related purposes [16]. Social media can also function as a hybrid forum for problem-structuring and data collection [16].

Specific technological applications for citizen engagement in technology-enabled community initiatives, which can be extrapolated to ADS governance, include sustainable technologies and gamification [3]. When designed to foster empowerment and align with user values, technology can act as a stimulus for participation [3].

Transparency mechanisms within ADS are also critical for enabling informed citizen engagement. While not strictly a method of direct participation, providing transparency, particularly through Algorithmic Demographic Information (ADI) that highlights the distribution of variables across demographic groups, can increase users' moral awareness of potential biases and influence their perceptions of fairness [1]. This awareness is a prerequisite for meaningful engagement on issues of algorithmic justice and fairness [1].

Despite the potential of digital tools to increase the breadth of participation, challenges remain. The digital divide can exclude populations lacking access to technology, skills, or digital literacy [16]. Ensuring inclusivity requires addressing these barriers and potentially using alternative technologies or offline methods [12][16]. Facilitators may still be needed to guide participants and ensure effective use of digital tools [16]. Furthermore, identifying and including diverse stakeholders and addressing power imbalances in digital processes are crucial for legitimacy [16].

Effective citizen engagement relies on building and maintaining trust, which is fundamental for motivating participation and commitment to outcomes [16]. Addressing diversity, equity, inclusion, and social justice is essential for establishing trust [16]. In online contexts, trust in the organizing entity is necessary for citizens to share information, highlighting the importance of data protection and voluntary consent [16].

When engaging large numbers of citizens, methods are needed to aggregate diverse inputs, which can range from qualitative approaches that consider different voices to quantitative mathematical methods for aggregating votes or preferences [16]. Deliberative methods, such as Citizens' Juries, can be adapted to engage the public in shaping data governance practices for new technologies, aligning with principles of Responsible Research and Innovation [2].

In the context of public sector ADM, public agencies have a responsibility to be transparent about the use, risks, and consequences of these systems [7]. Providing accessible information about algorithms, including overviews of their functions and regulatory compliance, empowers citizens and external actors to scrutinize and engage with the systems [7].

While digital tools offer significant potential for scaling citizen engagement, empirical research is still needed to fully understand their advantages and disadvantages in practice, particularly for broad participation at the societal level [16]. The choice of methods and tools ultimately depends on the specific context and purpose of the engagement in ADS governance [16].

Having explored various methods and technologies for citizen engagement, the following section will delve into the broader discussion of integrating these elements into effective governance frameworks.

# 11. Discussion

The preceding literature review has highlighted the multifaceted challenges and opportunities in governing automated decision systems (ADS) within the public sector, particularly concerning the tension between algorithmic efficiency and procedural justice, while striving to maintain democratic accountability and citizen trust. A key takeaway is that effective governance of public sector ADS requires a comprehensive approach that extends beyond technical fixes to address organizational practices, institutional frameworks, and the active involvement of citizens.

The literature underscores that algorithmic bias is not merely a technical issue but is deeply embedded in data, design choices, and the socio-technical context of implementation [1][9]. While transparency mechanisms like Feature Importance (FI) and Algorithmic Demographic Information (ADI) can increase awareness of potential discrimination [1], their impact on trust is complex and requires careful consideration [1]. This suggests that transparency alone is insufficient and must be coupled with other governance strategies.

The case studies, particularly the Australian Robodebt program, vividly illustrate the severe consequences of inadequate governance, including socially destructive outcomes and significant erosion of citizen trust [4]. These cases reveal critical deficiencies in existing legal frameworks and oversight mechanisms [4]. The concept of "multilayered blackboxing" further explains how organizational ignoring practices and institutional blindness can perpetuate algorithmic injustice, even with relatively simple systems [7]. This framework emphasizes that opacity is not limited to the algorithm itself but includes social and institutional layers that prevent actors from seeing and addressing errors [7].

Effectively implementing governance mechanisms for public sector ADS necessitates addressing these layers of blackboxing. Proposed frameworks call for actions across technological, organizational, extra-organizational, and institutional levels [7]. This includes ensuring open code and documentation, holding public agencies accountable for explaining ADM use and risks, providing external access to information, and empowering legal institutions to assess algorithm legality and provide recourse [7]. The Robodebt case highlights that overcoming managerial vision focused solely on efficiency, addressing limitations in human and technical capacity, and establishing robust internal and external oversight are critical for preventing destructive outcomes [4].

The implications for public policy are significant. Policymakers must move beyond viewing ADM as a purely technical tool for efficiency and recognize its profound social and legal impacts [7]. This requires adapting existing legal and regulatory frameworks to address the unique challenges of algorithmic decision-making, potentially including shifting the burden of proof and establishing mechanisms for systematic review and correction of algorithmic errors [7]. Investment in building capacity and competency within government organizations to manage ADM programs responsibly, including both technical and human resources with domain sensitivity, is also crucial [4]. Furthermore, policy should support mechanisms for meaningful citizen engagement, recognizing that democratic accountability requires involving the public in the design, deployment, and oversight of systems that affect their lives [16].

Citizen engagement methods and technologies, ranging from traditional facilitated workshops to digital platforms and open data initiatives, offer opportunities to integrate public perspectives into ADS governance [16]. However, challenges related to the digital divide, inclusivity, and power imbalances must be addressed to ensure equitable participation [16]. Building and maintaining trust is fundamental for motivating engagement and requires attention to diversity, equity, and inclusion [16].

Future research should delve deeper into the practical implementation and effectiveness of proposed governance frameworks in diverse public sector contexts. Empirical investigations are needed to understand how different governance mechanisms interact and influence the balance between efficiency and justice in practice. Further research is also required to explore how organizations can effectively integrate human judgment with automated systems in sensitive areas, determine responsibility in multi-stakeholder environments, and develop methods for aggregating diverse citizen inputs into actionable governance strategies [16][17]. Understanding the dynamic interplay between technical, organizational, and institutional factors that contribute to or mitigate algorithmic injustice remains a critical area for exploration.

Having discussed the key takeaways, implications, and future research directions, the following section will provide a concise summary of the report's findings and concluding remarks.

# 12. Conclusion

This report has explored the critical need for effective governance mechanisms to navigate the inherent tension between algorithmic efficiency and procedural justice in automated decision systems (ADS) within the public sector. The analysis of existing literature reveals that achieving this balance is essential for maintaining democratic accountability and citizen trust.

Key findings underscore that algorithmic bias is a pervasive issue stemming from data, design, and socio-technical factors, undermining the fairness of public sector ADS [1][9]. While transparency mechanisms are valuable for revealing potential discrimination, their impact on trust is complex, indicating that transparency alone is insufficient for robust governance [1]. The examination of accountability frameworks highlights the necessity of multi-layered approaches that address not only technical opacity but also organizational practices and institutional limitations [7]. Case studies, such as the Australian Robodebt program, provide stark evidence of the detrimental social and legal consequences that arise from inadequate governance and the limitations of existing legal and oversight mechanisms in addressing systematic algorithmic errors [4]. These cases demonstrate how organizational "ignoring practices" and institutional "blackboxing" can perpetuate injustice [7].

Furthermore, the report emphasizes the crucial role of citizen engagement in the governance of public sector ADS. Various methods and technologies exist to facilitate public participation, from traditional facilitated workshops to digital platforms [16]. However, challenges related to the digital divide, inclusivity, and power imbalances must be addressed to ensure equitable and meaningful engagement [16]. Building and maintaining trust is fundamental for motivating participation and requires deliberate attention to diversity, equity, and inclusion [16].

Based on these findings, several recommendations can be made for policymakers. Firstly, there is an urgent need to update existing legal and regulatory frameworks to explicitly address the unique challenges posed by algorithmic decision-making in the public sector. This should include mechanisms for systematic review and correction of algorithmic errors and potentially shifting the burden of proof regarding algorithmic legality onto public agencies [7]. Secondly, policymakers must prioritize building capacity and competency within government organizations to responsibly manage ADM programs, investing in both technical expertise and human resources with sufficient domain sensitivity [4]. Thirdly, policy should actively support and mandate meaningful citizen engagement in the design, deployment, and oversight of public sector ADS, ensuring that engagement methods are inclusive and address potential digital divides and power imbalances [16]. Finally, fostering a culture of transparency and accountability within public agencies that actively confronts ADM risks and avoids "organizational ignoring practices" is paramount [7].

This report highlights that the effective governance of public sector ADS is not merely a technical or legal challenge but a fundamental issue of democratic accountability and social justice. Ongoing research is essential to empirically evaluate the effectiveness of proposed governance frameworks in practice, explore the complex interplay between human judgment and automation, and develop robust methods for integrating diverse citizen inputs into governance processes. Continued policy development and implementation guided by these insights are critical to ensuring that automated decision systems serve the public good while upholding fundamental rights and democratic values.

## References

1. Sepideh Ebrahimi, Esraa Abdelhalim, Khaled Hassanein, Milena Head. (2024). Reducing the incidence of biased algorithmic decisions through feature importance transparency: an empirical study. *European Journal of Information Systems*.
2. V. Seymour, M. Xenitidou, L. Timotijevic, C. E. Hodgkins, E. Ratcliffe, B. Gatersleben, N. Gilbert, C. R. Jones. (2024). Public acceptance of smart home technologies in the UK: a citizens jury study. *Journal of Decision Systems*.
3. Catarina Neves, Tiago Oliveira, Saonee Sarker. (2024). Citizens participation in local energy communities: the role of technology as a stimulus. *European Journal of Information Systems*.
4. Tapani Rinta-Kahila, Ida Someh, Nicole Gillespie, Marta Indulska, Shirley Gregor. (2022). Algorithmic decision-making and system destructiveness: A case of automatic debt recovery. *European Journal of Information Systems*.
5. Michele Samorani, Shannon L. Harris, Linda Goler Blount, Haibing Lu, Michael A. Santoro. (2022). Overbooked and Overlooked: Machine Learning and Racial Bias in Medical Appointment Scheduling. *Manufacturing & Service Operations Management*.
6. Stephanie Kelley, Anton Ovchinnikov, David R. Hardoon, Adrienne Heinrich. (2022). Antidiscrimination Laws, Artificial Intelligence, and Gender Bias: A Case Study in Nonmortgage Fintech Lending. *Manufacturing & Service Operations Management*.
7. Charlotta Kronblad, Anna Essn, Magnus Mhring. (2024). When Justice is Blind to Algorithms: Multilayered Blackboxing of Algorithmic Decision Making in the Public Sector.
8. Nripendra P. Rana, Sheshadri Chatterjee, Yogesh K. Dwivedi, Shahriar Akter. (2022). Understanding dark side of artificial intelligence (AI) integrated business analytics: assessing firms operational inefficiency and competitiveness. *European Journal of Information Systems*.
9. Rohit Nishant, Dirk Schneckenberg, MN Ravishankar. (2024). The formal rationality of artificial intelligence-based algorithms and the problem of bias. *Journal of Information Technology*.
10. Jeroen Baijens, Tim Huygh, Remko Helms. (2022). Establishing and theorising data analytics governance: a descriptive framework and a VSM-based view. *Journal of Business Analytics*.
11. Atta Addo. (2022). Orchestrating a digital platform ecosystem to address societal challenges: A robust action perspective. *Journal of Information Technology*.
12. Carla Bonina, Kari Koskinen, Ben Eaton, Annabelle Gawer. (2021). Digital platforms for development: Foundations and research agenda. *Information Systems Journal*.
13. Ott Velsberg, Ulrika H. Westergren, Katrin Jonsson. (2020). Exploring smartness in public sector innovation - creating smart public services with the Internet of Things. *European Journal of Information Systems*.
14. Leif Sundberg, Jonny Holmstrm. (2024). Fusing domain knowledge with machine learning: A public sector perspective. *Journal of Strategic Information Systems*.
15. Jiaqi Yang, Alireza Amrollahi, Mauricio Marrone. (2024). Harnessing the Potential of Artificial Intelligence: Affordances, Constraints, and Strategic Implications for Professional Services. *Journal of Strategic Information Systems*.
16. Alice H. Aubert, Judit Lienert. (2024). Operational Research for, with, and by citizens: An overview. *European Journal of Operational Research*.
17. Crispin Coombs, Donald Hislop, Stanimira K. Taneva, Sarah Barnard. (2020). The strategic impacts of Intelligent Automation for knowledge and service work: An interdisciplinary review. *Journal of Strategic Information Systems*.